import json
import os
from datetime import datetime
from typing import List, Dict, Any, Optional
try:
    from supabase.client import create_client, Client
except ImportError:
    print("âŒ supabase íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. 'pip install supabase' ëª…ë ¹ì–´ë¡œ ì„¤ì¹˜í•´ì£¼ì„¸ìš”.")
    exit(1)
import asyncio
from pathlib import Path
from dotenv import load_dotenv
import glob

# .env íŒŒì¼ ë¡œë“œ
load_dotenv()

class SupabaseUploader:
    def __init__(self):
        """Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        # í™˜ê²½ ë³€ìˆ˜ì—ì„œ Supabase ì„¤ì • ê°€ì ¸ì˜¤ê¸°
        self.supabase_url = os.getenv('SUPABASE_URL')
        self.supabase_key = os.getenv('SUPABASE_ANON_KEY')
        
        if not self.supabase_url or not self.supabase_key:
            raise ValueError("SUPABASE_URLê³¼ SUPABASE_ANON_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
        
        try:
            self.supabase: Client = create_client(self.supabase_url, self.supabase_key)
        except Exception as e:
            print(f"âŒ Supabase í´ë¼ì´ì–¸íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            # ê°„ë‹¨í•œ ë°©ì‹ìœ¼ë¡œ ì¬ì‹œë„
            self.supabase = create_client(self.supabase_url, self.supabase_key)
        
        # ì–¸ë¡ ì‚¬ ë§¤í•‘ (í¬ë¡¤ëŸ¬ì—ì„œ ì‚¬ìš©í•˜ëŠ” ì´ë¦„ â†’ DBì˜ name)
        self.media_mapping = {
            "hani": "í•œê²¨ë ˆ",
            "chosun": "ì¡°ì„ ì¼ë³´", 
            "kbs": "KBS",
            "ytn": "YTN"
        }
        
        # ì–¸ë¡ ì‚¬ ID ìºì‹œ
        self.media_outlet_ids = {}
    
    def load_media_outlets(self):
        """ì–¸ë¡ ì‚¬ ì •ë³´ë¥¼ ë¡œë“œí•˜ì—¬ ID ë§¤í•‘ ìƒì„±"""
        try:
            response = self.supabase.table('media_outlets').select('id, name').execute()
            
            for outlet in response.data:
                self.media_outlet_ids[outlet['name']] = outlet['id']
            
            print(f"âœ… ì–¸ë¡ ì‚¬ ì •ë³´ ë¡œë“œ ì™„ë£Œ: {list(self.media_outlet_ids.keys())}")
            
        except Exception as e:
            print(f"âŒ ì–¸ë¡ ì‚¬ ì •ë³´ ë¡œë“œ ì‹¤íŒ¨: {e}")
            raise
    
    def load_json_file(self, file_path: str) -> List[Dict[str, Any]]:
        """JSON íŒŒì¼ ë¡œë“œ"""
        try:
            with open(file_path, 'r', encoding='utf-8') as f:
                data = json.load(f)
            
            print(f"âœ… JSON íŒŒì¼ ë¡œë“œ ì™„ë£Œ: {file_path} ({len(data)}ê°œ ê¸°ì‚¬)")
            return data
            
        except Exception as e:
            print(f"âŒ JSON íŒŒì¼ ë¡œë“œ ì‹¤íŒ¨ ({file_path}): {e}")
            return []
    
    def prepare_article_data(self, articles: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ê¸°ì‚¬ ë°ì´í„°ë¥¼ Supabase í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        prepared_data = []
        
        for article in articles:
            try:
                # ì–¸ë¡ ì‚¬ ID ì°¾ê¸°
                source = article.get('source', '')
                # í¬ë¡¤ëŸ¬ ì´ë¦„ì„ DB ì´ë¦„ìœ¼ë¡œ ë§¤í•‘
                mapped_source = self.media_mapping.get(source, source)
                media_outlet_id = self.media_outlet_ids.get(mapped_source)
                
                if not media_outlet_id:
                    print(f"âš ï¸ ì–¸ë¡ ì‚¬ IDë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ: {source}")
                    continue
                
                # ë°ì´í„° ë³€í™˜
                prepared_article = {
                    'title': article.get('title', ''),
                    'url': article.get('url', ''),
                    'content': article.get('content', ''),
                    'media_outlet_id': media_outlet_id,
                    'category': article.get('category', ''),
                    'published_at': self.parse_datetime(article.get('crawled_at')),
                    'created_at': datetime.now().isoformat()
                }
                
                # í•„ìˆ˜ í•„ë“œ ê²€ì¦
                if not prepared_article['title'] or not prepared_article['url']:
                    print(f"âš ï¸ í•„ìˆ˜ í•„ë“œ ëˆ„ë½: {article}")
                    continue
                
                prepared_data.append(prepared_article)
                
            except Exception as e:
                print(f"âŒ ê¸°ì‚¬ ë°ì´í„° ë³€í™˜ ì‹¤íŒ¨: {e}")
                continue
        
        return prepared_data
    
    def parse_datetime(self, date_str: Optional[str]) -> str:
        """ë‚ ì§œ ë¬¸ìì—´ì„ ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜"""
        if not date_str:
            return datetime.now().isoformat()
        
        try:
            # ë‹¤ì–‘í•œ ë‚ ì§œ í˜•ì‹ ì§€ì›
            formats = [
                '%Y-%m-%dT%H:%M:%S',
                '%Y-%m-%d %H:%M:%S',
                '%Y-%m-%d',
            ]
            
            for fmt in formats:
                try:
                    dt = datetime.strptime(date_str, fmt)
                    return dt.isoformat()
                except ValueError:
                    continue
            
            # íŒŒì‹± ì‹¤íŒ¨ ì‹œ í˜„ì¬ ì‹œê°„ ë°˜í™˜
            return datetime.now().isoformat()
            
        except Exception:
            return datetime.now().isoformat()
    
    def upload_articles(self, articles: List[Dict[str, Any]], batch_size: int = 100) -> Dict[str, int]:
        """ê¸°ì‚¬ ë°ì´í„°ë¥¼ Supabaseì— ì—…ë¡œë“œ"""
        total_articles = len(articles)
        uploaded_count = 0
        failed_count = 0
        
        print(f"ğŸ“¤ {total_articles}ê°œ ê¸°ì‚¬ ì—…ë¡œë“œ ì‹œì‘...")
        
        # ë°°ì¹˜ ë‹¨ìœ„ë¡œ ì—…ë¡œë“œ
        for i in range(0, total_articles, batch_size):
            batch = articles[i:i + batch_size]
            
            try:
                # ì¤‘ë³µ ì²´í¬ (URL ê¸°ë°˜)
                urls = [article['url'] for article in batch]
                existing_response = self.supabase.table('articles').select('url').in_('url', urls).execute()
                existing_urls = {item['url'] for item in existing_response.data}
                
                # ì¤‘ë³µë˜ì§€ ì•Šì€ ê¸°ì‚¬ë§Œ í•„í„°ë§
                new_articles = [article for article in batch if article['url'] not in existing_urls]
                
                if not new_articles:
                    print(f"âš ï¸ ë°°ì¹˜ {i//batch_size + 1}: ëª¨ë“  ê¸°ì‚¬ê°€ ì´ë¯¸ ì¡´ì¬í•¨")
                    continue
                
                # ì—…ë¡œë“œ ì‹¤í–‰
                response = self.supabase.table('articles').insert(new_articles).execute()
                
                batch_uploaded = len(new_articles)
                uploaded_count += batch_uploaded
                
                print(f"âœ… ë°°ì¹˜ {i//batch_size + 1}: {batch_uploaded}ê°œ ê¸°ì‚¬ ì—…ë¡œë“œ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ ë°°ì¹˜ {i//batch_size + 1} ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                failed_count += len(batch)
                continue
        
        return {
            'total': total_articles,
            'uploaded': uploaded_count,
            'failed': failed_count,
            'skipped': total_articles - uploaded_count - failed_count
        }
    
    def upload_from_json_files(self, data_dir: str = "../crawler/data/raw") -> Dict[str, Any]:
        """JSON íŒŒì¼ë“¤ì„ ì½ì–´ì„œ Supabaseì— ì—…ë¡œë“œ"""
        # ì–¸ë¡ ì‚¬ ì •ë³´ ë¡œë“œ
        self.load_media_outlets()
        
        # JSON íŒŒì¼ ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
        data_path = Path(data_dir)
        json_files = list(data_path.glob("*.json"))
        
        if not json_files:
            print(f"âŒ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {data_dir}")
            return {}
        
        print(f"ğŸ“ {len(json_files)}ê°œ JSON íŒŒì¼ ë°œê²¬: {[f.name for f in json_files]}")
        
        total_results = {
            'files_processed': 0,
            'total_articles': 0,
            'uploaded_articles': 0,
            'failed_articles': 0,
            'skipped_articles': 0,
            'results_by_source': {}
        }
        
        # ê° JSON íŒŒì¼ ì²˜ë¦¬
        for json_file in json_files:
            print(f"\nğŸ”„ ì²˜ë¦¬ ì¤‘: {json_file.name}")
            
            # JSON ë°ì´í„° ë¡œë“œ
            articles = self.load_json_file(str(json_file))
            if not articles:
                continue
            
            # ë°ì´í„° ë³€í™˜
            prepared_articles = self.prepare_article_data(articles)
            if not prepared_articles:
                print(f"âš ï¸ ë³€í™˜ëœ ê¸°ì‚¬ê°€ ì—†ìŠµë‹ˆë‹¤: {json_file.name}")
                continue
            
            # ì—…ë¡œë“œ ì‹¤í–‰
            result = self.upload_articles(prepared_articles)
            
            # ê²°ê³¼ ì§‘ê³„
            source_name = json_file.stem.replace('_20250705', '')  # íŒŒì¼ëª…ì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ
            total_results['results_by_source'][source_name] = result
            total_results['files_processed'] += 1
            total_results['total_articles'] += result['total']
            total_results['uploaded_articles'] += result['uploaded']
            total_results['failed_articles'] += result['failed']
            total_results['skipped_articles'] += result['skipped']
        
        return total_results
    
    def print_summary(self, results: Dict[str, Any]):
        """ì—…ë¡œë“œ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        print("\n" + "="*50)
        print("ğŸ“Š ì—…ë¡œë“œ ê²°ê³¼ ìš”ì•½")
        print("="*50)
        
        print(f"ì²˜ë¦¬ëœ íŒŒì¼: {results['files_processed']}ê°œ")
        print(f"ì´ ê¸°ì‚¬ ìˆ˜: {results['total_articles']}ê°œ")
        print(f"ì—…ë¡œë“œ ì„±ê³µ: {results['uploaded_articles']}ê°œ")
        print(f"ì—…ë¡œë“œ ì‹¤íŒ¨: {results['failed_articles']}ê°œ")
        print(f"ì¤‘ë³µ ìŠ¤í‚µ: {results['skipped_articles']}ê°œ")
        
        if results['total_articles'] > 0:
            success_rate = (results['uploaded_articles'] / results['total_articles']) * 100
            print(f"ì„±ê³µë¥ : {success_rate:.1f}%")
        
        print("\nğŸ“‹ ì–¸ë¡ ì‚¬ë³„ ê²°ê³¼:")
        for source, result in results['results_by_source'].items():
            print(f"  {source}: {result['uploaded']}/{result['total']}ê°œ ì—…ë¡œë“œ")

    def upload_clusters_from_json(self, json_path: Optional[str] = None):
        """í´ëŸ¬ìŠ¤í„° ê²°ê³¼ JSONì„ ì½ì–´ issues, issue_articles í…Œì´ë¸”ì— ì—…ë¡œë“œ"""
        import json
        from pathlib import Path
        if json_path is None:
            # backend/results/ì—ì„œ ê°€ì¥ ìµœê·¼ *_final.json ì‚¬ìš©
            result_dir = Path(__file__).parent / 'results'
            json_files = sorted(result_dir.glob('*_final.json'), key=lambda x: x.stat().st_mtime, reverse=True)
            if not json_files:
                print('âŒ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ JSON íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.')
                return
            json_path = str(json_files[0])
        print(f'ğŸ“‚ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ íŒŒì¼: {json_path}')
        with open(json_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        clusters = data.get('clusters', [])
        if not clusters:
            print('âŒ í´ëŸ¬ìŠ¤í„° ë°ì´í„°ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤.')
            return
        # ì—…ë¡œë“œ ì‹œì‘
        self.load_media_outlets()  # í˜¹ì‹œ í•„ìš”í•  ìˆ˜ ìˆìœ¼ë‹ˆ í˜¸ì¶œ
        issues_uploaded = 0
        issue_articles_uploaded = 0
        for cluster in clusters:
            # issues í…Œì´ë¸”ì— ì €ì¥
            title_val = cluster.get('title')
            summary_val = cluster.get('summary')
            title = str(title_val) if title_val is not None else ''
            summary = str(summary_val) if summary_val is not None else ''
            
            # titleì´ None, ë¹ˆ ë¬¸ìì—´, ê³µë°±ë§Œ ìˆì„ ê²½ìš° summary ì•ë¶€ë¶„ìœ¼ë¡œ ëŒ€ì²´
            if not title or title.strip() == '':
                # summaryì—ì„œ ì²« ë²ˆì§¸ ë¬¸ì¥ì´ë‚˜ ì˜ë¯¸ ìˆëŠ” ë¶€ë¶„ ì¶”ì¶œ
                if summary:
                    # ë§ˆì¹¨í‘œë‚˜ ëŠë‚Œí‘œ, ë¬¼ìŒí‘œë¡œ ëë‚˜ëŠ” ì²« ë²ˆì§¸ ë¬¸ì¥ ì¶”ì¶œ
                    import re
                    sentences = re.split(r'[.!?]', summary)
                    if sentences and sentences[0].strip():
                        title = sentences[0].strip()[:50]  # 50ìë¡œ ì œí•œ
                    else:
                        title = summary[:50].strip()  # 50ìë¡œ ì œí•œ
                else:
                    title = f"{cluster.get('category', 'ê¸°íƒ€')} ê´€ë ¨ ì´ìŠˆ"
            
            # ì œëª©ì´ ì—¬ì „íˆ ë¹„ì–´ìˆë‹¤ë©´ ê¸°ë³¸ê°’ ì„¤ì •
            if not title or title.strip() == '':
                title = f"{cluster.get('category', 'ê¸°íƒ€')} ê´€ë ¨ ì´ìŠˆ #{issues_uploaded + 1}"
            
            issue_insert = {
                'category': cluster.get('category', 'ê¸°íƒ€'),
                'title': title,
                'summary': summary,
                'article_count': cluster.get('article_count', 0),
                'bias_left': cluster.get('bias_left', 0),
                'bias_center': cluster.get('bias_center', 0),
                'bias_right': cluster.get('bias_right', 0),
            }
            try:
                issue_resp = self.supabase.table('issues').insert(issue_insert).execute()
                if not issue_resp.data or not issue_resp.data[0].get('id'):
                    print(f"âŒ ì´ìŠˆ ì—…ë¡œë“œ ì‹¤íŒ¨: {issue_insert}")
                    continue
                issue_id = issue_resp.data[0]['id']
                issues_uploaded += 1
                print(f"âœ… ì´ìŠˆ ì—…ë¡œë“œ ì™„ë£Œ: {title[:30]}...")
            except Exception as e:
                print(f"âŒ ì´ìŠˆ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                continue
            # issue_articles í…Œì´ë¸”ì— ì €ì¥
            for article_id in cluster.get('article_ids', []):
                try:
                    ia_insert = {'issue_id': issue_id, 'article_id': article_id}
                    self.supabase.table('issue_articles').insert(ia_insert).execute()
                    issue_articles_uploaded += 1
                except Exception as e:
                    print(f"âŒ issue_articles ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
                    continue
        print(f"\nâœ… ì´ìŠˆ {issues_uploaded}ê°œ, ì´ìŠˆ-ê¸°ì‚¬ ë§¤í•‘ {issue_articles_uploaded}ê°œ ì—…ë¡œë“œ ì™„ë£Œ!")


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    import sys
    try:
        print("ğŸ”§ í™˜ê²½ ë³€ìˆ˜ ì„¤ì • í™•ì¸...")
        print("SUPABASE_URLê³¼ SUPABASE_ANON_KEYê°€ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”.")
        print()
        uploader = SupabaseUploader()
        
        # ëª…ì‹œì ìœ¼ë¡œ í´ëŸ¬ìŠ¤í„° JSON íŒŒì¼ì´ ì§€ì •ëœ ê²½ìš°
        if len(sys.argv) > 1:
            cluster_json_path = sys.argv[1]
            print(f"\nğŸ§  ì§€ì •ëœ í´ëŸ¬ìŠ¤í„°(ì´ìŠˆ) ê²°ê³¼ë¥¼ DBì— ì—…ë¡œë“œí•©ë‹ˆë‹¤: {cluster_json_path}")
            uploader.upload_clusters_from_json(cluster_json_path)
            return True
        
        # í¬ë¡¤ë§ ë°ì´í„° ì—…ë¡œë“œ (ì‹¤íŒ¨í•´ë„ ê³„ì† ì§„í–‰)
        try:
            results = uploader.upload_from_json_files()
            uploader.print_summary(results)
        except Exception as e:
            print(f"âš ï¸ í¬ë¡¤ë§ ë°ì´í„° ì—…ë¡œë“œ ì‹¤íŒ¨(ë¬´ì‹œ): {e}")
        # í´ëŸ¬ìŠ¤í„° ê²°ê³¼ ì—…ë¡œë“œëŠ” í•­ìƒ ì‹¤í–‰
        print("\nğŸ§  í´ëŸ¬ìŠ¤í„°(ì´ìŠˆ) ê²°ê³¼ë„ DBì— ì—…ë¡œë“œí•©ë‹ˆë‹¤...")
        uploader.upload_clusters_from_json()
    except Exception as e:
        print(f"âŒ ì „ì²´ ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    return True


if __name__ == "__main__":
    success = main()
    exit(0 if success else 1) 