import os
import pandas as pd
from supabase.client import create_client
from dotenv import load_dotenv
from openai import OpenAI
from sklearn.cluster import DBSCAN
import numpy as np
import json
from tqdm import tqdm
from datetime import datetime
import argparse
import tiktoken
import subprocess
import glob

# 1. í™˜ê²½ ë³€ìˆ˜ ë¡œë“œ ë° ì„¤ì •
load_dotenv()
SUPABASE_URL = os.getenv('SUPABASE_URL')
SUPABASE_KEY = os.getenv('SUPABASE_ANON_KEY')
OPENAI_API_KEY = os.getenv('OPENAI_API_KEY')

if not SUPABASE_URL or not SUPABASE_KEY:
    raise ValueError("SUPABASE_URLê³¼ SUPABASE_ANON_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")
if not OPENAI_API_KEY:
    raise ValueError("OPENAI_API_KEY í™˜ê²½ ë³€ìˆ˜ë¥¼ ì„¤ì •í•´ì£¼ì„¸ìš”.")

client = OpenAI(api_key=OPENAI_API_KEY)
supabase = create_client(SUPABASE_URL, SUPABASE_KEY)

# 2. DBì—ì„œ ê¸°ì‚¬ ë°ì´í„° ë¶ˆëŸ¬ì˜¤ê¸° (media_outlet_id í¬í•¨)
def fetch_articles():
    response = supabase.table('articles').select('id, title, content, category, media_outlet_id, url').execute()
    df = pd.DataFrame(response.data)
    print(f"âœ… {len(df)}ê°œ ê¸°ì‚¬ ë¡œë“œ ì™„ë£Œ")
    
    # ì¤‘ë³µ ì œê±° ì ìš©
    df_deduped = remove_duplicate_articles(df)
    print(f"ğŸ§¹ ì¤‘ë³µ ì œê±° í›„: {len(df_deduped)}ê°œ ê¸°ì‚¬ ({len(df) - len(df_deduped)}ê°œ ì¤‘ë³µ ì œê±°)")
    
    return df_deduped

# ì–¸ë¡ ì‚¬ ì •ë³´ ë¡œë“œ (idâ†’name, idâ†’bias)
def fetch_media_outlets():
    response = supabase.table('media_outlets').select('id, name, bias').execute()
    mapping = {}
    for row in response.data:
        mapping[row['id']] = {'name': row['name'], 'bias': row['bias']}
    return mapping

# ì¤‘ë³µ ê¸°ì‚¬ ì œê±° í•¨ìˆ˜ë“¤
def calculate_title_similarity(title1, title2):
    """ì œëª© ê°„ ìœ ì‚¬ë„ ê³„ì‚° (Jaccard ìœ ì‚¬ë„ ì‚¬ìš©)"""
    import re
    
    # íŠ¹ìˆ˜ë¬¸ì ì œê±°í•˜ê³  ë‹¨ì–´ ë‹¨ìœ„ë¡œ ë¶„ë¦¬
    def tokenize(text):
        text = re.sub(r'[^\w\s]', '', str(text).lower())
        return set(text.split())
    
    tokens1 = tokenize(title1)
    tokens2 = tokenize(title2)
    
    if len(tokens1) == 0 and len(tokens2) == 0:
        return 1.0
    if len(tokens1) == 0 or len(tokens2) == 0:
        return 0.0
    
    intersection = len(tokens1.intersection(tokens2))
    union = len(tokens1.union(tokens2))
    
    return intersection / union if union > 0 else 0.0

def remove_duplicate_articles(df, title_similarity_threshold=0.8):
    """
    ì¤‘ë³µ ê¸°ì‚¬ ì œê±°:
    1. URL ê¸°ë°˜ ì™„ì „ ì¤‘ë³µ ì œê±°
    2. ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±° (ê°™ì€ ì´ìŠˆë¥¼ ë‹¤ë£¬ ê¸°ì‚¬ë“¤)
    """
    if len(df) == 0:
        return df
    
    print(f"ğŸ” ì¤‘ë³µ ì œê±° ì‹œì‘ (ì´ {len(df)}ê°œ ê¸°ì‚¬)")
    
    # 1ë‹¨ê³„: URL ê¸°ë°˜ ì™„ì „ ì¤‘ë³µ ì œê±°
    df_url_deduped = df.drop_duplicates(subset=['url'], keep='first')
    url_removed = len(df) - len(df_url_deduped)
    if url_removed > 0:
        print(f"   ğŸ“ URL ì¤‘ë³µ ì œê±°: {url_removed}ê°œ")
    
    # 2ë‹¨ê³„: ì¹´í…Œê³ ë¦¬ë³„ë¡œ ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì¤‘ë³µ ì œê±°
    result_dfs = []
    
    for category in df_url_deduped['category'].unique():
        if pd.isna(category):
            continue
            
        cat_df = df_url_deduped[df_url_deduped['category'] == category].copy()
        if len(cat_df) <= 1:
            result_dfs.append(cat_df)
            continue
        
        print(f"   ğŸ—‚ï¸ [{category}] ì¹´í…Œê³ ë¦¬: {len(cat_df)}ê°œ ê¸°ì‚¬")
        
        # ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ê·¸ë£¹í•‘
        to_remove = set()
        articles = cat_df.reset_index(drop=True)
        
        for i in range(len(articles)):
            if i in to_remove:
                continue
                
            current_title = articles.loc[i, 'title']
            current_media = articles.loc[i, 'media_outlet_id']
            
            # ì´í›„ ê¸°ì‚¬ë“¤ê³¼ ë¹„êµ
            for j in range(i+1, len(articles)):
                if j in to_remove:
                    continue
                    
                compare_title = articles.loc[j, 'title']
                compare_media = articles.loc[j, 'media_outlet_id']
                
                # ê°™ì€ ì–¸ë¡ ì‚¬ë©´ ìŠ¤í‚µ (ì–¸ë¡ ì‚¬ ë‚´ ì¤‘ë³µì€ URLë¡œ ì´ë¯¸ ì²˜ë¦¬ë¨)
                if current_media == compare_media:
                    continue
                
                similarity = calculate_title_similarity(current_title, compare_title)
                
                if similarity >= title_similarity_threshold:
                    # ìœ ì‚¬í•œ ê¸°ì‚¬ ë°œê²¬ - ë” ê¸´ ë³¸ë¬¸ì„ ê°€ì§„ ê¸°ì‚¬ë¥¼ ìœ ì§€
                    current_content_len = len(str(articles.loc[i, 'content']))
                    compare_content_len = len(str(articles.loc[j, 'content']))
                    
                    if compare_content_len > current_content_len:
                        to_remove.add(i)
                        print(f"      ğŸ”„ ìœ ì‚¬ ì œëª© ë°œê²¬ (ìœ ì‚¬ë„: {similarity:.2f})")
                        print(f"         ì œê±°: {current_title[:50]}...")
                        print(f"         ìœ ì§€: {compare_title[:50]}...")
                        break
                    else:
                        to_remove.add(j)
                        print(f"      ğŸ”„ ìœ ì‚¬ ì œëª© ë°œê²¬ (ìœ ì‚¬ë„: {similarity:.2f})")
                        print(f"         ìœ ì§€: {current_title[:50]}...")
                        print(f"         ì œê±°: {compare_title[:50]}...")
        
        # ì œê±°í•  ê¸°ì‚¬ë“¤ ì œì™¸í•˜ê³  ë‚¨ì€ ê¸°ì‚¬ë“¤ ì¶”ê°€
        filtered_articles = articles.drop(index=list(to_remove))
        result_dfs.append(filtered_articles)
        
        title_removed = len(to_remove)
        if title_removed > 0:
            print(f"      â¡ï¸ ì œëª© ìœ ì‚¬ë„ ê¸°ë°˜ ì œê±°: {title_removed}ê°œ")
    
    # ëª¨ë“  ì¹´í…Œê³ ë¦¬ ê²°ê³¼ í•©ì¹˜ê¸°
    final_df = pd.concat(result_dfs, ignore_index=True) if result_dfs else pd.DataFrame()
    
    total_removed = len(df) - len(final_df)
    print(f"âœ… ì´ {total_removed}ê°œ ì¤‘ë³µ ê¸°ì‚¬ ì œê±° ì™„ë£Œ")
    
    return final_df

# 3. ì„ë² ë”© ìƒì„± (OpenAI text-embedding-3-small)
def get_embeddings(texts, model="text-embedding-3-small", cache=None, batch_size=256):
    embeddings = []
    if cache is None:
        cache = {}
    n = len(texts)
    for i in range(0, n, batch_size):
        batch = texts[i:i+batch_size]
        batch_embeddings = []
        uncached_idx = []
        uncached_texts = []
        # ìºì‹œ í™•ì¸
        for idx, text in enumerate(batch):
            if text in cache:
                batch_embeddings.append(cache[text])
            else:
                batch_embeddings.append(None)
                uncached_idx.append(idx)
                uncached_texts.append(text)
        # uncachedë§Œ batchë¡œ ì„ë² ë”© ìš”ì²­
        if uncached_texts:
            try:
                res = client.embeddings.create(input=uncached_texts, model=model)
                for j, emb in enumerate(res.data):
                    cache[uncached_texts[j]] = emb.embedding
                    batch_embeddings[uncached_idx[j]] = emb.embedding
            except Exception as e:
                print(f"âŒ ì„ë² ë”© ì‹¤íŒ¨(batch): {e}")
                for j in uncached_idx:
                    batch_embeddings[j] = [0.0]*1536
        embeddings.extend(batch_embeddings)
    return np.array(embeddings)

# 4. í´ëŸ¬ìŠ¤í„°ë§ (DBSCAN)
def cluster_embeddings(embeddings, eps=0.5, min_samples=3):
    db = DBSCAN(eps=eps, min_samples=min_samples, metric='cosine')
    labels = db.fit_predict(embeddings)
    return labels

# 5. í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ì´ìŠˆ ìš”ì•½ (gpt-3.5-turbo)
def summarize_cluster(df_cluster, model="gpt-3.5-turbo", max_tokens=300, max_articles=12, chunk_size=6000):
    enc = tiktoken.encoding_for_model(model)
    # 1. ì–¸ë¡ ì‚¬/í¸í–¥ë³„ ê³ ë¥¸ ìƒ˜í”Œë§
    sampled = []
    # ìš°ì„  biasë³„ ê·¸ë£¹í•‘
    for bias in ['left', 'center', 'right']:
        group = df_cluster[df_cluster['media_outlet_id'].notnull() & (df_cluster['media_outlet_id'] != '')]
        group = group[df_cluster['bias'] == bias] if 'bias' in df_cluster else df_cluster
        group = group.sample(min(len(group), max_articles//3), random_state=42) if len(group) > 0 else pd.DataFrame()
        sampled.append(group)
    sampled_df = pd.concat(sampled) if sampled else df_cluster.sample(min(len(df_cluster), max_articles), random_state=42)
    # 2. ê¸°ì‚¬ë³„ë¡œ ì œëª©+ë³¸ë¬¸ ì¼ë¶€ë§Œ ì¶”ì¶œ
    def make_text(row):
        title = str(row['title'])
        content = str(row['content'])[:1000]
        return f"[ì œëª©] {title}\n[ë³¸ë¬¸] {content}"
    texts = [make_text(row) for _, row in sampled_df.iterrows()]
    # 3. context ì´ˆê³¼ ë°©ì§€: í† í° ê¸¸ì´ ì²´í¬ ë° ìë¥´ê¸°
    prompt_head = "ë‹¤ìŒ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì„ ëŒ€í‘œí•˜ëŠ” ì´ìŠˆì— ëŒ€í•´, ì²˜ìŒ ë³´ëŠ” ì‚¬ëŒë„ ì´í•´í•  ìˆ˜ ìˆë„ë¡ 4~5ë¬¸ë‹¨ ì´ìƒì˜ í’ë¶€í•œ ì„¤ëª…, ë°°ê²½, ì£¼ìš” ìŸì ê¹Œì§€ í¬í•¨í•´ ìì„¸íˆ ìš”ì•½í•´ì¤˜:\n"
    joined = "\n\n".join(texts)
    tokens = len(enc.encode(prompt_head + joined))
    # ë„ˆë¬´ ê¸¸ë©´ chunkë³„ ë¶€ë¶„ ìš”ì•½ í›„ í•©ì¹˜ê¸°
    if tokens > 14000:
        # chunk ë‹¨ìœ„ë¡œ ë‚˜ëˆ  ë¶€ë¶„ ìš”ì•½
        chunked = []
        cur = []
        cur_tokens = 0
        for t in texts:
            t_tokens = len(enc.encode(t))
            if cur_tokens + t_tokens > chunk_size:
                chunked.append(cur)
                cur = []
                cur_tokens = 0
            cur.append(t)
            cur_tokens += t_tokens
        if cur:
            chunked.append(cur)
        partial_summaries = []
        for chunk in chunked:
            prompt = prompt_head + "\n\n".join(chunk)
            try:
                res = client.chat.completions.create(
                    model=model,
                    messages=[{"role": "user", "content": prompt}],
                    max_tokens=max_tokens,
                    temperature=0.4
                )
                content = getattr(res.choices[0].message, "content", None)
                if content:
                    partial_summaries.append(content.strip())
            except Exception as e:
                print(f"âŒ ë¶€ë¶„ ìš”ì•½ ì‹¤íŒ¨: {e}")
        # ë¶€ë¶„ ìš”ì•½ í•©ì³ì„œ ìµœì¢… ìš”ì•½
        final_prompt = prompt_head + "\n\n".join(partial_summaries)
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": final_prompt}],
                max_tokens=max_tokens,
                temperature=0.4
            )
            content = getattr(res.choices[0].message, "content", None)
            if content:
                return content.strip()
            else:
                return "ìš”ì•½ ì‹¤íŒ¨"
        except Exception as e:
            print(f"âŒ ìµœì¢… ìš”ì•½ ì‹¤íŒ¨: {e}")
            return "ìš”ì•½ ì‹¤íŒ¨"
    else:
        prompt = prompt_head + joined
        try:
            res = client.chat.completions.create(
                model=model,
                messages=[{"role": "user", "content": prompt}],
                max_tokens=max_tokens,
                temperature=0.4
            )
            content = getattr(res.choices[0].message, "content", None)
            if content:
                return content.strip()
            else:
                return "ìš”ì•½ ì‹¤íŒ¨"
        except Exception as e:
            print(f"âŒ ìš”ì•½ ì‹¤íŒ¨: {e}")
            return "ìš”ì•½ ì‹¤íŒ¨"

# 5-1. í´ëŸ¬ìŠ¤í„° ëŒ€í‘œ ì œëª© ìƒì„± (GPTê°€ ì§ê´€ì ì´ê³  í¥ë¯¸ë¡œìš´ ì œëª© ìƒì„±)
def generate_cluster_title(df_cluster, model="gpt-3.5-turbo", max_tokens=100, max_articles=8):
    """
    í´ëŸ¬ìŠ¤í„°ì— í¬í•¨ëœ ê¸°ì‚¬ë“¤ì„ ë¶„ì„í•´ì„œ GPTê°€ ì§ê´€ì ì´ê³  í¥ë¯¸ë¡œìš´ ì œëª©ì„ ìƒì„±
    """
    enc = tiktoken.encoding_for_model(model)
    
    # 1. ëŒ€í‘œ ê¸°ì‚¬ë“¤ ìƒ˜í”Œë§ (ë„ˆë¬´ ë§ìœ¼ë©´ í† í° ì´ˆê³¼)
    sampled_df = df_cluster.sample(min(len(df_cluster), max_articles), random_state=42)
    
    # 2. ê¸°ì‚¬ë³„ë¡œ ì œëª©+ë³¸ë¬¸ ìš”ì•½ ì¶”ì¶œ
    def make_text(row):
        title = str(row['title'])
        content = str(row['content'])[:500]  # ì œëª© ìƒì„±ìš©ì´ë¯€ë¡œ ì§§ê²Œ
        return f"â€¢ {title}\n  ìš”ì•½: {content[:200]}..."
    
    texts = [make_text(row) for _, row in sampled_df.iterrows()]
    
    # 3. ì œëª© ì „ìš© í”„ë¡¬í”„íŠ¸ (summaryì™€ í™•ì‹¤íˆ êµ¬ë¶„)
    prompt = f"""ë‹¤ìŒì€ ê°™ì€ ì´ìŠˆì— ê´€í•œ ë‰´ìŠ¤ ê¸°ì‚¬ë“¤ì…ë‹ˆë‹¤:

{chr(10).join(texts)}

ìœ„ ê¸°ì‚¬ë“¤ì˜ í•µì‹¬ ì´ìŠˆë¥¼ í•œëˆˆì— íŒŒì•…í•  ìˆ˜ ìˆëŠ” **ì„íŒ©íŠ¸ ìˆëŠ” í—¤ë“œë¼ì¸**ì„ ë§Œë“¤ì–´ì£¼ì„¸ìš”.

ì¡°ê±´:
- 15-25ìì˜ ì§§ê³  ê°•ë ¬í•œ ì œëª©
- í´ë¦­í•˜ê³  ì‹¶ê²Œ ë§Œë“œëŠ” í‘œí˜„ (ê¶ê¸ˆì¦, ê°ì • ìê·¹)
- "!", "?", "..." ë“± íŠ¹ìˆ˜ë¬¸ìë¡œ ê°•ì¡°
- í•µì‹¬ í‚¤ì›Œë“œ ìœ„ì£¼ë¡œ ê°„ê²°í•˜ê²Œ
- ë‰´ìŠ¤ í—¤ë“œë¼ì¸ ìŠ¤íƒ€ì¼ (ìƒì„¸ ì„¤ëª… ê¸ˆì§€)

ë°˜ë“œì‹œ ì œëª©ë§Œ ì¶œë ¥í•˜ì„¸ìš”:"""

    # 4. í† í° ê¸¸ì´ ì²´í¬
    tokens = len(enc.encode(prompt))
    if tokens > 3000:  # ë„ˆë¬´ ê¸¸ë©´ ê¸°ì‚¬ ìˆ˜ ì¤„ì´ê¸°
        sampled_df = df_cluster.sample(min(len(df_cluster), 4), random_state=42)
        texts = [make_text(row) for _, row in sampled_df.iterrows()]
        prompt = f"""ë‹¤ìŒ ê¸°ì‚¬ë“¤ì˜ í•µì‹¬ ì´ìŠˆë¥¼ ë‚˜íƒ€ë‚´ëŠ” ì„íŒ©íŠ¸ ìˆëŠ” í—¤ë“œë¼ì¸ì„ 15-25ìë¡œ ìƒì„±í•˜ì„¸ìš”:

{chr(10).join(texts)}

ì§§ê³  ê°•ë ¬í•œ ì œëª©ë§Œ ì¶œë ¥:"""

    # 5. GPT í˜¸ì¶œ
    try:
        res = client.chat.completions.create(
            model=model,
            messages=[{"role": "user", "content": prompt}],
            max_tokens=max_tokens,
            temperature=0.7  # ì°½ì˜ì ì¸ ì œëª©ì„ ìœ„í•´ ì¡°ê¸ˆ ë†’ê²Œ
        )
        content = getattr(res.choices[0].message, "content", None)
        if content:
            # ìƒì„±ëœ ì œëª© ì •ë¦¬ (ë”°ì˜´í‘œ, ë¶ˆí•„ìš”í•œ ë¬¸ì ì œê±°)
            title = content.strip().strip('"').strip("'").strip()
            
            # ì œëª©ì´ ë„ˆë¬´ ê¸¸ë©´ ìë¥´ê¸°
            if len(title) > 50:
                title = title[:47] + "..."
            
            return title if title else "ìƒˆë¡œìš´ ì´ìŠˆ ë°œìƒ"
        else:
            return "ìƒˆë¡œìš´ ì´ìŠˆ ë°œìƒ"
    except Exception as e:
        print(f"âŒ ì œëª© ìƒì„± ì‹¤íŒ¨: {e}")
        # í´ë°±: ê¸°ì‚¬ ì œëª©ì—ì„œ í•µì‹¬ í‚¤ì›Œë“œ ì¶”ì¶œ (summaryì™€ ì™„ì „íˆ ë‹¤ë¥¸ ë°©ì‹)
        if len(df_cluster) > 0:
            # ê°€ì¥ ëŒ€í‘œì ì¸ ê¸°ì‚¬ ì œëª©ì„ ê¸°ë°˜ìœ¼ë¡œ ì„íŒ©íŠ¸ ìˆëŠ” ì œëª© ìƒì„±
            original_title = str(df_cluster['title'].iloc[0])
            
            # í•µì‹¬ ë‹¨ì–´ ì¶”ì¶œì„ ìœ„í•œ ê°„ë‹¨í•œ ì²˜ë¦¬
            keywords = []
            common_words = ['ê¸°ì', 'ë‰´ìŠ¤', 'ì¼ë³´', 'ë°©ì†¡', 'ì·¨ì¬', 'ë³´ë„', 'ë°œí‘œ', 'ê¸°ì‚¬']
            
            words = original_title.split()
            for word in words:
                if len(word) > 1 and word not in common_words:
                    keywords.append(word)
                    if len(keywords) >= 3:  # ìµœëŒ€ 3ê°œ í‚¤ì›Œë“œ
                        break
            
            if keywords:
                fallback_title = ' '.join(keywords[:2])  # ìƒìœ„ 2ê°œ í‚¤ì›Œë“œë§Œ
                if len(fallback_title) > 25:
                    fallback_title = fallback_title[:22] + "..."
                return fallback_title + " ì´ìŠˆ!"
            else:
                return original_title[:20] + "... ì£¼ëª©!"
        return "ê¸´ê¸‰ ì´ìŠˆ ë°œìƒ!"

# 6. ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
def main(output_path=None, eps_list=None, min_samples_list=None):
    df = fetch_articles()
    # ì„ë² ë”© ì…ë ¥ ê¸¸ì´ ì œí•œ(3000ì)
    df['text'] = (df['title'] + '\n' + df['content'].fillna('')).str.slice(0, 3000)
    media_map = fetch_media_outlets()
    categories = df['category'].unique()
    # íŒŒë¼ë¯¸í„° grid
    if eps_list is None:
        eps_list = [0.3, 0.4, 0.5, 0.6, 0.7]
    if min_samples_list is None:
        min_samples_list = [2, 3, 4, 5]
    # ì €ì¥ ê²½ë¡œ ìë™ ìƒì„±
    if output_path is None:
        now_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_path = f"{now_str}_grid.json"
    all_results = []
    summary_lines = []
    summary_lines.append("| eps | min_samples | category | clusters | noise | total |")
    summary_lines.append("|-----|-------------|----------|----------|-------|-------|")
    # ì „ì²´ ê¸°ì‚¬ ì„ë² ë”© ìºì‹± (í…ìŠ¤íŠ¸: ë²¡í„°)
    embedding_cache = {}
    # ì¹´í…Œê³ ë¦¬ë³„ í…ìŠ¤íŠ¸ ë¯¸ë¦¬ ì¤€ë¹„
    cat_texts = {cat: df[df['category'] == cat]['text'].tolist() for cat in categories}
    cat_embeddings = {}
    for category in categories:
        texts = cat_texts[category]
        if len(texts) == 0:
            cat_embeddings[category] = np.zeros((0, 1536))
        else:
            print(f"ğŸ§  [{category}] ì „ì²´ ì„ë² ë”© ìƒì„± ì¤‘... (ì¤‘ë³µ ì œê±°, batch)")
            cat_embeddings[category] = get_embeddings(texts, cache=embedding_cache)
    for eps in eps_list:
        for min_samples in min_samples_list:
            print(f"\n==================== [eps={eps}, min_samples={min_samples}] ====================")
            meta = []
            cards = []
            clusters_json = []
            for category in categories:
                print(f"\nğŸ—‚ï¸  ===== [ì¹´í…Œê³ ë¦¬: {category}] =====")
                texts = cat_texts[category]
                embeddings = cat_embeddings[category]
                if len(texts) == 0:
                    print("(í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ ì—†ìŒ)")
                    continue
                print(f"ğŸ”— í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
                labels = cluster_embeddings(embeddings, eps=eps, min_samples=min_samples)
                n_total = len(labels)
                n_noise = sum(1 for l in labels if l == -1)
                n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
                print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {n_clusters} | noise(ì´ìƒì¹˜): {n_noise} | ì „ì²´ ê¸°ì‚¬: {n_total}")
                meta.append({
                    'category': category,
                    'n_clusters': n_clusters,
                    'n_noise': n_noise,
                    'n_total': n_total
                })
                # í´ëŸ¬ìŠ¤í„°ë³„ ì¹´ë“œ ìƒì„±
                for cluster_id in sorted(set(labels)):
                    if cluster_id == -1:
                        continue  # noise ì œì™¸
                    idxs = [i for i, l in enumerate(labels) if l == cluster_id]
                    df_cluster = df[(df['category'] == category)].iloc[idxs]
                    bias_counter = {'left': 0, 'center': 0, 'right': 0}
                    for _, row in df_cluster.iterrows():
                        media_info = media_map.get(row['media_outlet_id'], {'name': 'Unknown', 'bias': 'unknown'})
                        bias = media_info['bias']
                        if bias in bias_counter:
                            bias_counter[bias] += 1
                    total = sum(bias_counter.values())
                    bias_percent = {k: (v, v/total*100 if total else 0) for k, v in bias_counter.items()}
                    bar = 'ğŸŸ¥'*int(bias_percent['left'][1]//5) + 'â¬œ'*int(bias_percent['center'][1]//5) + 'ğŸŸ¦'*int(bias_percent['right'][1]//5)
                    # GPTê°€ í´ëŸ¬ìŠ¤í„° ë‚´ìš©ì„ ë¶„ì„í•´ì„œ ì§ê´€ì ì´ê³  í¥ë¯¸ë¡œìš´ ì œëª© ìƒì„±
                    print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„° #{cluster_id+1} ì œëª© ìƒì„± ì¤‘...")
                    cluster_title = generate_cluster_title(df_cluster)
                    print(f"âœ¨ ìƒì„±ëœ ì œëª©: {cluster_title}")
                    
                    summary = summarize_cluster(df_cluster)
                    card_lines = []
                    card_lines.append(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: **{category}** | í´ëŸ¬ìŠ¤í„° #{cluster_id+1}\n")
                    card_lines.append(f"### ğŸ·ï¸ ì´ìŠˆ ì œëª©\n**{cluster_title}**\n")
                    card_lines.append("========================================\n")
                    card_lines.append(f"#### ğŸ§  ëŒ€í‘œ ì´ìŠˆ ìš”ì•½\n\n**{summary}**\n")
                    card_lines.append("========================================\n")
                    card_lines.append(f"#### ğŸ“° í¬í•¨ëœ ê¸°ì‚¬ë“¤ (ì´ {len(df_cluster)}ê°œ)\n")
                    card_lines.append(f"#### ğŸ“Š í¸í–¥ ê²Œì´ì§€\n{bar}\nleft: {bias_counter['left']} ({round(bias_percent['left'][1],1)}%) | center: {bias_counter['center']} ({round(bias_percent['center'][1],1)}%) | right: {bias_counter['right']} ({round(bias_percent['right'][1],1)}%)\n")
                    card_text = "\n".join(card_lines)
                    cards.append({'category': category, 'card_text': card_text})
                    # JSONìš© í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
                    clusters_json.append({
                        'category': category,
                        'title': cluster_title,
                        'summary': summary,
                        'article_count': len(df_cluster),
                        'bias_left': bias_counter['left'],
                        'bias_center': bias_counter['center'],
                        'bias_right': bias_counter['right'],
                        'article_ids': df_cluster['id'].tolist()
                    })
                summary_lines.append(f"| {eps} | {min_samples} | {category} | {n_clusters} | {n_noise} | {n_total} |")
            all_results.append({
                'eps': eps,
                'min_samples': min_samples,
                'meta': meta,
                'cards': cards
            })
    with open(output_path, "w", encoding="utf-8") as f:
        json.dump({'results': all_results}, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… ìë™ íŒŒë¼ë¯¸í„° íƒìƒ‰ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    # summary_table.txtë¡œ í‘œ ì €ì¥
    with open("summary_table.txt", "w", encoding="utf-8") as f:
        f.write("\n".join(summary_lines))
    print("âœ… íŒŒë¼ë¯¸í„°ë³„ í´ëŸ¬ìŠ¤í„°ë§ ê²°ê³¼ ìš”ì•½í‘œê°€ summary_table.txtì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")
    # JSON ê²°ê³¼ë„ í•¨ê»˜ ì €ì¥
    json_output_path = output_path.replace('.json', '.json')
    with open(json_output_path, "w", encoding="utf-8") as f:
        json.dump({'clusters': clusters_json}, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… ìµœì¢…ë³¸(ì´ìŠˆ ì¹´ë“œ TXT)ì´ {output_path}ì—, JSONì´ {json_output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

def main_single(output_path=None, eps=0.3, min_samples=3):
    df = fetch_articles()
    df['text'] = (df['title'] + '\n' + df['content'].fillna('')).str.slice(0, 3000)
    media_map = fetch_media_outlets()
    categories = df['category'].unique()
    if output_path is None:
        now_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = "backend/results"
        os.makedirs(output_dir, exist_ok=True)
        output_path = os.path.join(output_dir, f"{now_str}_final.txt")
    # ì„ë² ë”© ìºì‹± ë° batch ì²˜ë¦¬
    embedding_cache = {}
    cat_texts = {cat: df[df['category'] == cat]['text'].tolist() for cat in categories}
    cat_embeddings = {}
    for category in categories:
        texts = cat_texts[category]
        if len(texts) == 0:
            cat_embeddings[category] = np.zeros((0, 1536))
        else:
            print(f"ğŸ§  [{category}] ì „ì²´ ì„ë² ë”© ìƒì„± ì¤‘... (ì¤‘ë³µ ì œê±°, batch)")
            cat_embeddings[category] = get_embeddings(texts, cache=embedding_cache)
    cards = []
    clusters_json = []
    for category in categories:
        print(f"\nğŸ—‚ï¸  ===== [ì¹´í…Œê³ ë¦¬: {category}] =====")
        texts = cat_texts[category]
        embeddings = cat_embeddings[category]
        if len(texts) == 0:
            print("(í•´ë‹¹ ì¹´í…Œê³ ë¦¬ ê¸°ì‚¬ ì—†ìŒ)")
            continue
        print(f"ğŸ”— í´ëŸ¬ìŠ¤í„°ë§ ì¤‘...")
        labels = cluster_embeddings(embeddings, eps=eps, min_samples=min_samples)
        n_total = len(labels)
        n_noise = sum(1 for l in labels if l == -1)
        n_clusters = len(set(labels)) - (1 if -1 in labels else 0)
        print(f"ğŸ“Š í´ëŸ¬ìŠ¤í„° ê°œìˆ˜: {n_clusters} | noise(ì´ìƒì¹˜): {n_noise} | ì „ì²´ ê¸°ì‚¬: {n_total}")
        # í´ëŸ¬ìŠ¤í„°ë³„ ì¹´ë“œ ìƒì„±
        for cluster_id in sorted(set(labels)):
            if cluster_id == -1:
                continue  # noise ì œì™¸
            idxs = [i for i, l in enumerate(labels) if l == cluster_id]
            df_cluster = df[(df['category'] == category)].iloc[idxs]
            bias_counter = {'left': 0, 'center': 0, 'right': 0}
            for _, row in df_cluster.iterrows():
                media_info = media_map.get(row['media_outlet_id'], {'name': 'Unknown', 'bias': 'unknown'})
                bias = media_info['bias']
                if bias in bias_counter:
                    bias_counter[bias] += 1
            total = sum(bias_counter.values())
            bias_percent = {k: (v, v/total*100 if total else 0) for k, v in bias_counter.items()}
            bar = 'ğŸŸ¥'*int(bias_percent['left'][1]//5) + 'â¬œ'*int(bias_percent['center'][1]//5) + 'ğŸŸ¦'*int(bias_percent['right'][1]//5)
            # GPTê°€ í´ëŸ¬ìŠ¤í„° ë‚´ìš©ì„ ë¶„ì„í•´ì„œ ì§ê´€ì ì´ê³  í¥ë¯¸ë¡œìš´ ì œëª© ìƒì„±
            print(f"ğŸ¯ í´ëŸ¬ìŠ¤í„° #{cluster_id+1} ì œëª© ìƒì„± ì¤‘...")
            cluster_title = generate_cluster_title(df_cluster)
            print(f"âœ¨ ìƒì„±ëœ ì œëª©: {cluster_title}")
            
            summary = summarize_cluster(df_cluster)
            card_lines = []
            card_lines.append(f"ğŸ·ï¸ ì¹´í…Œê³ ë¦¬: **{category}** | í´ëŸ¬ìŠ¤í„° #{cluster_id+1}\n")
            card_lines.append(f"### ğŸ·ï¸ ì´ìŠˆ ì œëª©\n**{cluster_title}**\n")
            card_lines.append("========================================\n")
            card_lines.append(f"#### ğŸ§  ëŒ€í‘œ ì´ìŠˆ ìš”ì•½\n\n**{summary}**\n")
            card_lines.append("========================================\n")
            card_lines.append(f"#### ğŸ“° í¬í•¨ëœ ê¸°ì‚¬ë“¤ (ì´ {len(df_cluster)}ê°œ)\n")
            card_lines.append(f"#### ğŸ“Š í¸í–¥ ê²Œì´ì§€\n{bar}\nleft: {bias_counter['left']} ({round(bias_percent['left'][1],1)}%) | center: {bias_counter['center']} ({round(bias_percent['center'][1],1)}%) | right: {bias_counter['right']} ({round(bias_percent['right'][1],1)}%)\n")
            card_text = "\n".join(card_lines)
            cards.append({'category': category, 'card_text': card_text})
            # JSONìš© í´ëŸ¬ìŠ¤í„° ì •ë³´ ì¶”ê°€
            clusters_json.append({
                'category': category,
                'title': cluster_title,
                'summary': summary,
                'article_count': len(df_cluster),
                'bias_left': bias_counter['left'],
                'bias_center': bias_counter['center'],
                'bias_right': bias_counter['right'],
                'article_ids': df_cluster['id'].tolist()
            })
    with open(output_path, "w", encoding="utf-8") as f:
        for card in cards:
            f.write(card['card_text'])
            f.write("\n" + ("-"*40) + "\n")
    # JSON ê²°ê³¼ë„ í•¨ê»˜ ì €ì¥
    json_output_path = output_path.replace('.txt', '.json')
    with open(json_output_path, "w", encoding="utf-8") as f:
        json.dump({'clusters': clusters_json}, f, ensure_ascii=False, indent=2)
    print(f"\nâœ… ìµœì¢…ë³¸(ì´ìŠˆ ì¹´ë“œ TXT)ì´ {output_path}ì—, JSONì´ {json_output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument('output_path', nargs='?', default=None, help='ì €ì¥í•  JSON ê²½ë¡œ')
    parser.add_argument('--single', action='store_true', default=True, help='ì¶”ì²œ ì¡°í•©(ìµœì¢…ë³¸)ë§Œ ì €ì¥')
    args = parser.parse_args()
    if args.single:
        main_single(args.output_path)
    else:
        main(args.output_path)

    # === ìë™ DB ì—…ë¡œë“œ ===
    # ë°©ê¸ˆ ìƒì„±ëœ ìµœì‹  JSON íŒŒì¼ì„ ì§ì ‘ ì „ë‹¬
    if args.single:
        # single ëª¨ë“œì—ì„œëŠ” ë°©ê¸ˆ ìƒì„±ëœ JSON íŒŒì¼ì„ ì‚¬ìš©
        if args.output_path:
            latest_json = args.output_path.replace('.txt', '.json')
        else:
            # output_pathê°€ Noneì´ë©´ main_singleì—ì„œ ìƒì„±ëœ íŒŒì¼ ê²½ë¡œ ì°¾ê¸°
            result_dir = "backend/results"
            json_files = sorted(glob.glob(os.path.join(result_dir, '*_final.json')), key=os.path.getmtime, reverse=True)
            latest_json = json_files[0] if json_files else None
    else:
        # multi ëª¨ë“œì—ì„œëŠ” ìµœì‹  íŒŒì¼ ì°¾ê¸°
        result_dir = os.path.join(os.path.dirname(__file__), 'results')
        json_files = sorted(glob.glob(os.path.join(result_dir, '*_final.json')), key=os.path.getmtime, reverse=True)
        latest_json = json_files[0] if json_files else None
    
    if latest_json and os.path.exists(latest_json):
        print(f"\nğŸŸ¢ í´ëŸ¬ìŠ¤í„° ê²°ê³¼ë¥¼ DBì— ìë™ ì—…ë¡œë“œí•©ë‹ˆë‹¤: {latest_json}")
        try:
            # ê¸°ì¡´ ì´ìŠˆ ë°ì´í„° ì •ë¦¬ í›„ ìµœì‹  ê²°ê³¼ ì—…ë¡œë“œ
            subprocess.run(['python3', 'supabase_uploader.py', latest_json], check=True)
        except Exception as e:
            print(f"âŒ DB ì—…ë¡œë“œ ìë™ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
    else:
        print("âŒ ìë™ ì—…ë¡œë“œìš© í´ëŸ¬ìŠ¤í„° ê²°ê³¼ JSON íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.") 