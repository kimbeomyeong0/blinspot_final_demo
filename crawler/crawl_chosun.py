# crawler/crawl_chosun.py

import os
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime
from utils.parser_common import get_html, clean_text
from playwright.async_api import async_playwright
import logging

logger = logging.getLogger(__name__)

CATEGORY_URLS = {
    "ì •ì¹˜": [
        "https://www.chosun.com/politics/",
        "https://www.chosun.com/politics/?page=1",
        "https://www.chosun.com/politics/?page=2",
        "https://www.chosun.com/politics/?page=3"
    ],
    "ì‚¬íšŒ": [
        "https://www.chosun.com/national/",
        "https://www.chosun.com/national/?page=1", 
        "https://www.chosun.com/national/?page=2",
        "https://www.chosun.com/national/?page=3"
    ],
    "ê²½ì œ": [
        "https://www.chosun.com/economy/",
        "https://www.chosun.com/economy/?page=1",
        "https://www.chosun.com/economy/?page=2", 
        "https://www.chosun.com/economy/?page=3"
    ],
}

async def extract_article_content(page, article_url):
    """ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        # ê¸°ì¡´ í˜ì´ì§€ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ
        await page.goto(article_url, wait_until="domcontentloaded", timeout=20000)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        await page.wait_for_timeout(500)
        
        html = await page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        # ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ë³¸ë¬¸ ì…€ë ‰í„° ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœ)
        content_selectors = [
            '.article-body',
            '.news-article-body', 
            '.story-content',
            '.entry-content',
            '#article-body',
            '.article-content',
            '.text-content',
            '.content-body',
            '.article-text',
            '.story-body'
        ]
        
        content = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = clean_text(content_elem.get_text())
                if len(content) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ë©´ ì¦‰ì‹œ ë°˜í™˜
                    break
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
        if not content or len(content.strip()) < 50:
            content = "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        return content
        
    except Exception as e:
        logger.error(f"ì¡°ì„ ì¼ë³´ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - {article_url}: {e}")
        return "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

async def get_articles():
    all_articles = []
    
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ìµœì í™” ì„¤ì •
        browser = await p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-images',
                '--disable-web-security',
                '--disable-features=TranslateUI',
                '--disable-renderer-backgrounding',
                '--disable-background-timer-throttling',
                '--disable-default-apps',
                '--disable-sync',
                '--disable-translate',
                '--hide-scrollbars',
                '--mute-audio',
                '--no-first-run',
                '--no-default-browser-check',
                '--disable-component-update',
                '--disable-domain-reliability',
                '--disable-print-preview',
                '--disable-speech-api',
                '--disable-web-bluetooth',
                '--disable-client-side-phishing-detection',
                '--disable-hang-monitor',
                '--disable-prompt-on-repost',
                '--disable-breakpad',
                '--disable-dev-tools',
                '--disable-in-process-stack-traces',
                '--disable-histogram-customizer',
                '--disable-gl-extensions',
                '--disable-3d-apis',
                '--disable-accelerated-2d-canvas',
                '--disable-accelerated-jpeg-decoding',
                '--disable-accelerated-mjpeg-decode',
                '--disable-accelerated-video-decode',
            ]
        )
        
        for category, urls in CATEGORY_URLS.items():
            try:
                print(f"ğŸ” ì¡°ì„ ì¼ë³´ {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
                
                count = 0
                target_count = 30
                processed_urls = set()
                
                for url_idx, url in enumerate(urls):
                    if count >= target_count:
                        break
                        
                    try:
                        print(f"ğŸ“„ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}/{len(urls)} ì²˜ë¦¬ ì¤‘...")
                        
                        page = await browser.new_page()
                        
                        # í˜ì´ì§€ ìµœì í™” ì„¤ì •
                        await page.set_extra_http_headers({
                            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                            'Cache-Control': 'no-cache'
                        })
                        
                        # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ (ì„±ëŠ¥ í–¥ìƒ)
                        await page.route('**/*.{png,jpg,jpeg,gif,svg,ico,webp,css,woff,woff2,ttf,eot}', 
                                       lambda route: route.abort())
                        
                        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                        
                        click_count = 0
                        max_clicks = 35  # ë”ë³´ê¸° í´ë¦­ íšŸìˆ˜ ëŒ€í­ ì¦ê°€
                        
                        while count < target_count and click_count < max_clicks:
                            # í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ìˆ˜ì§‘
                            html = await page.content()
                            soup = BeautifulSoup(html, "html.parser")
                            
                            # ì¡°ì„ ì¼ë³´ ê¸°ì‚¬ ëª©ë¡ ì…€ë ‰í„° (ë” í¬ê´„ì ìœ¼ë¡œ)
                            selectors = [
                                "div.story-card a",
                                ".story-card a",
                                ".article-card a",
                                ".news-card a",
                                "article a",
                                ".headline a",
                                ".story-item a",
                                ".news-item a",
                                "a[href*='/article/']",
                                "a[href*='/news/']",
                                ".list-item a",
                                ".news-list a",
                                ".article-list a",
                                "a[href*='/politics/']",
                                "a[href*='/national/']",
                                "a[href*='/economy/']"
                            ]
                            
                            nodes = []
                            for selector in selectors:
                                nodes = soup.select(selector)
                                if nodes:
                                    if click_count == 0:  # ì²« ë²ˆì§¸ì—ì„œë§Œ ë¡œê·¸ ì¶œë ¥
                                        print(f"ğŸ“„ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}ì—ì„œ '{selector}' ì…€ë ‰í„°ë¡œ {len(nodes)}ê°œ ìš”ì†Œ ë°œê²¬")
                                    break
                            
                            if not nodes:
                                print(f"âŒ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}: ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                                break
                            
                            # í˜„ì¬ ë¡œë“œëœ ê¸°ì‚¬ë“¤ ì²˜ë¦¬
                            new_articles_found = 0
                            for node in nodes:
                                if count >= target_count:
                                    break
                                    
                                try:
                                    link = node.get('href')
                                    title = clean_text(node.get_text())
                                    
                                    if not link or not title or len(title) < 3:
                                        continue
                                    
                                    # linkê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                                    if isinstance(link, list):
                                        link = link[0] if link else ""
                                    
                                    # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                                    if link.startswith('/'):
                                        link = f"https://www.chosun.com{link}"
                                    elif not link.startswith('http'):
                                        link = f"https://www.chosun.com/{link}"
                                    
                                    # ì¤‘ë³µ ì œê±°
                                    if link in processed_urls:
                                        continue
                                    
                                    if any(article['url'] == link for article in all_articles):
                                        continue
                                    
                                    processed_urls.add(link)
                                    
                                    # ë³¸ë¬¸ ì¶”ì¶œ
                                    print(f"ğŸ“„ [{count+1}/{target_count}] {title} - ë³¸ë¬¸ ì¶”ì¶œ ì¤‘...")
                                    content = await extract_article_content(page, article_url=link)
                                    
                                    article = {
                                        'title': title,
                                        'url': link,
                                        'category': category,
                                        'content': content,
                                        'published_at': datetime.now().isoformat(),
                                        'source': 'chosun'
                                    }
                                    
                                    all_articles.append(article)
                                    count += 1
                                    new_articles_found += 1
                                    
                                    print(f"âœ… [{count}/{target_count}] {title[:50]}... (ë³¸ë¬¸ {len(content)}ì)")
                                    
                                except Exception as e:
                                    print(f"âŒ ì¡°ì„ ì¼ë³´ {category} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                    continue
                            
                            # ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                            if new_articles_found == 0 and click_count > 0:
                                print(f"  ğŸ“„ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}: ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì—†ì–´ ì¤‘ë‹¨")
                                break
                            
                            # "ê¸°ì‚¬ ë”ë³´ê¸°" ë²„íŠ¼ ì°¾ê¸° ë° í´ë¦­ (ë” ì ê·¹ì ìœ¼ë¡œ)
                            try:
                                # ë‹¤ì–‘í•œ ë”ë³´ê¸° ë²„íŠ¼ ì…€ë ‰í„° ì‹œë„
                                more_selectors = [
                                    "button:has-text('ê¸°ì‚¬ ë”ë³´ê¸°')",
                                    "a:has-text('ê¸°ì‚¬ ë”ë³´ê¸°')",
                                    "button:has-text('ë”ë³´ê¸°')",
                                    "a:has-text('ë”ë³´ê¸°')",
                                    ".more-btn",
                                    ".btn-more",
                                    "#more-btn",
                                    "button.more",
                                    "a.more",
                                    "[data-more]",
                                    "[onclick*='more']",
                                    "button:has-text('More')",
                                    ".load-more",
                                    ".btn-load-more",
                                    ".more-articles",
                                    ".load-articles",
                                    "button[data-load]",
                                    "a[data-load]",
                                    ".paging .next",
                                    ".pagination .next",
                                    "a:has-text('ë‹¤ìŒ')",
                                    "button:has-text('ë‹¤ìŒ')",
                                    ".next-page",
                                    ".page-next"
                                ]
                                
                                more_button = None
                                for selector in more_selectors:
                                    try:
                                        more_button = page.locator(selector).first
                                        if await more_button.is_visible():
                                            break
                                    except:
                                        continue
                                
                                if more_button and await more_button.is_visible():
                                    print(f"  ğŸ”„ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}: ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ({click_count + 1}ë²ˆì§¸)")
                                    await more_button.click()
                                    click_count += 1
                                    
                                    # ìƒˆ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
                                    await page.wait_for_timeout(4000)  # ë” ê¸´ ëŒ€ê¸° ì‹œê°„
                                else:
                                    # ë”ë³´ê¸° ë²„íŠ¼ì´ ì—†ìœ¼ë©´ ìŠ¤í¬ë¡¤ ì‹œë„
                                    print(f"  ğŸ”„ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}: ìŠ¤í¬ë¡¤ ì‹œë„ ({click_count + 1}ë²ˆì§¸)")
                                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                    await page.wait_for_timeout(3000)
                                    
                                    # í˜ì´ì§€ ëê¹Œì§€ ìŠ¤í¬ë¡¤í–ˆëŠ”ì§€ í™•ì¸
                                    await page.evaluate("window.scrollTo(0, document.body.scrollHeight)")
                                    await page.wait_for_timeout(2000)
                                    
                                    # ìƒˆë¡œìš´ ì½˜í…ì¸ ê°€ ë¡œë“œë˜ì—ˆëŠ”ì§€ í™•ì¸
                                    new_html = await page.content()
                                    new_soup = BeautifulSoup(new_html, "html.parser")
                                    new_nodes = []
                                    for selector in selectors:
                                        new_nodes = new_soup.select(selector)
                                        if new_nodes:
                                            break
                                    
                                    if len(new_nodes) <= len(nodes):
                                        print(f"  ğŸ“„ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}: ê¸°ì‚¬ ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ì–´ ì¤‘ë‹¨")
                                        break
                                    
                                    click_count += 1
                                    
                            except Exception as e:
                                print(f"  âŒ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}: ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì¤‘ ì˜¤ë¥˜: {e}")
                                break
                        
                        await page.close()
                        print(f"âœ… ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1}ì—ì„œ {click_count}ë²ˆ í´ë¦­ ì™„ë£Œ")
                        
                    except Exception as e:
                        print(f"âŒ ì¡°ì„ ì¼ë³´ {category} URL {url_idx + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        if 'page' in locals():
                            await page.close()
                        continue
                
                print(f"âœ… ì¡°ì„ ì¼ë³´ {category}ì—ì„œ {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ Error crawling ì¡°ì„ ì¼ë³´ {category}: {e}")
                if 'page' in locals():
                    await page.close()
                continue
        
        await browser.close()
    
    print(f"âœ… ì¡°ì„ ì¼ë³´ì—ì„œ ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_articles

if __name__ == "__main__":
    asyncio.run(get_articles())