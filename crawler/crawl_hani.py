# crawler/crawl_hani.py

import os
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime
from utils.parser_common import get_html, clean_text
from playwright.async_api import async_playwright
import logging

logger = logging.getLogger(__name__)

CATEGORY_URLS = {
    "ì •ì¹˜": "https://www.hani.co.kr/arti/politics",
    "ì‚¬íšŒ": "https://www.hani.co.kr/arti/society",
    "ê²½ì œ": "https://www.hani.co.kr/arti/economy",
}

async def extract_article_content(page, article_url):
    """í•œê²¨ë ˆ ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        # ê¸°ì¡´ í˜ì´ì§€ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ
        await page.goto(article_url, wait_until="domcontentloaded", timeout=20000)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        await page.wait_for_timeout(500)
        
        html = await page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        # í•œê²¨ë ˆ ê¸°ì‚¬ ë³¸ë¬¸ ì…€ë ‰í„° ì‹œë„
        selectors = [
            ".ArticleText",  # í•œê²¨ë ˆ ì£¼ìš” ë³¸ë¬¸
            ".article-text",
            ".article-body", 
            ".news-article-body",
            ".story-content",
            ".entry-content",
            "#article-body",
            ".article-content",
            ".text",
            ".content"
        ]
        
        content = ""
        for selector in selectors:
            element = soup.select_one(selector)
            if element:
                content = clean_text(element.get_text())
                if len(content) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ì˜ ë³¸ë¬¸ì´ ìˆìœ¼ë©´ ì¦‰ì‹œ ë°˜í™˜
                    break
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
        if len(content) < 50:
            content = "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        return content
        
    except Exception as e:
        logger.error(f"í•œê²¨ë ˆ ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - {article_url}: {e}")
        return "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

async def get_articles():
    all_articles = []
    
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ìµœì í™” ì„¤ì •
        browser = await p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-images',
                '--disable-web-security',
                '--disable-features=TranslateUI',
                '--disable-renderer-backgrounding',
                '--disable-background-timer-throttling',
                '--disable-default-apps',
                '--disable-sync',
                '--disable-translate',
                '--hide-scrollbars',
                '--mute-audio',
                '--no-first-run',
                '--no-default-browser-check',
                '--disable-component-update',
                '--disable-domain-reliability',
                '--disable-print-preview',
                '--disable-speech-api',
                '--disable-web-bluetooth',
                '--disable-client-side-phishing-detection',
                '--disable-hang-monitor',
                '--disable-prompt-on-repost',
                '--disable-breakpad',
                '--disable-dev-tools',
                '--disable-in-process-stack-traces',
                '--disable-histogram-customizer',
                '--disable-gl-extensions',
                '--disable-3d-apis',
                '--disable-accelerated-2d-canvas',
                '--disable-accelerated-jpeg-decoding',
                '--disable-accelerated-mjpeg-decode',
                '--disable-accelerated-video-decode',
            ]
        )
        
        for category, url in CATEGORY_URLS.items():
            try:
                print(f"ğŸ” í•œê²¨ë ˆ {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
                
                page = await browser.new_page()
                
                # í˜ì´ì§€ ìµœì í™” ì„¤ì •
                await page.set_extra_http_headers({
                    'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                    'Cache-Control': 'no-cache'
                })
                
                # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ (ì„±ëŠ¥ í–¥ìƒ)
                await page.route('**/*.{png,jpg,jpeg,gif,svg,ico,webp,css,woff,woff2,ttf,eot}', 
                               lambda route: route.abort())
                
                await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                
                # í˜ì´ì§€ê°€ ì™„ì „íˆ ë¡œë“œë  ë•Œê¹Œì§€ ëŒ€ê¸°
                await page.wait_for_selector('li.ArticleList_item___OGQO', timeout=30000)
                
                count = 0
                target_count = 30
                page_num = 1
                max_pages = 20  # í˜ì´ì§€ ìˆ˜ ì¦ê°€
                
                while count < target_count and page_num <= max_pages:
                    try:
                        print(f"ğŸ“„ í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num} ì²˜ë¦¬ ì¤‘...")
                        
                        # í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ë§í¬ë“¤ ìˆ˜ì§‘
                        try:
                            await page.wait_for_selector('li.ArticleList_item___OGQO a', timeout=15000)
                        except:
                            # ì²« ë²ˆì§¸ í˜ì´ì§€ê°€ ì•„ë‹ˆë©´ ë‹¤ë¥¸ ë°©ë²•ìœ¼ë¡œ ì‹œë„
                            if page_num > 1:
                                # URL ì§ì ‘ ì¡°ì‘ìœ¼ë¡œ í˜ì´ì§€ ì´ë™
                                base_url = url
                                if '?' in base_url:
                                    page_url = f"{base_url}&page={page_num}"
                                else:
                                    page_url = f"{base_url}?page={page_num}"
                                
                                print(f"ğŸ“„ í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num} ì§ì ‘ URL ì ‘ê·¼: {page_url}")
                                await page.goto(page_url, wait_until="domcontentloaded", timeout=30000)
                                await page.wait_for_timeout(3000)
                                
                                try:
                                    await page.wait_for_selector('li.ArticleList_item___OGQO a', timeout=10000)
                                except:
                                    print(f"âŒ í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num}: ê¸°ì‚¬ ëª©ë¡ ì—†ìŒ")
                                    break
                        
                        # í˜ì´ì§€ HTML ê°€ì ¸ì˜¤ê¸°
                        html = await page.content()
                        soup = BeautifulSoup(html, 'html.parser')
                        
                        # ê¸°ì‚¬ ë§í¬ ì¶”ì¶œ (ë” í¬ê´„ì ìœ¼ë¡œ)
                        article_selectors = [
                            'li.ArticleList_item___OGQO a',
                            '.article-list a',
                            '.news-list a',
                            'article a',
                            '.headline a',
                            'a[href*="/arti/"]'
                        ]
                        
                        article_links = []
                        for selector in article_selectors:
                            article_links = soup.select(selector)
                            if article_links:
                                break
                        
                        if not article_links:
                            print(f"âš ï¸ í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num}: ê¸°ì‚¬ ë§í¬ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                            break
                        
                        print(f"ğŸ“„ í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num}ì—ì„œ {len(article_links)}ê°œ ê¸°ì‚¬ ë°œê²¬")
                        
                        # ê° ê¸°ì‚¬ ë§í¬ ì²˜ë¦¬
                        processed_in_page = 0
                        for link in article_links:
                            if count >= target_count:
                                break
                                
                            href = link.get('href')
                            if not href:
                                continue
                            
                            # hrefê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                            if isinstance(href, list):
                                href = href[0] if href else ""
                                
                            # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                            if href.startswith('/'):
                                article_url = f"https://www.hani.co.kr{href}"
                            else:
                                article_url = href
                            
                            # ì¤‘ë³µ ì²´í¬
                            if any(article['url'] == article_url for article in all_articles):
                                continue
                            
                            # ì œëª© ì¶”ì¶œ
                            title_elem = link.select_one('.BaseArticleCard_title__TVFqt')
                            if not title_elem:
                                continue
                                
                            title = clean_text(title_elem.get_text().strip())
                            
                            # ì œëª© ê¸¸ì´ ì²´í¬ (ë„ˆë¬´ ì§§ì€ ì œëª© ì œì™¸)
                            if len(title) < 3:
                                continue
                            
                            # ë³¸ë¬¸ ì¶”ì¶œ
                            print(f"ğŸ“„ [{count+1}/{target_count}] {title} - ë³¸ë¬¸ ì¶”ì¶œ ì¤‘...")
                            content = await extract_article_content(page, article_url)
                            
                            # ê¸°ì‚¬ ì •ë³´ ì €ì¥
                            article_info = {
                                'title': title,
                                'url': article_url,
                                'category': category,
                                'content': content,
                                'source': 'hani',
                                'published_at': datetime.now().isoformat()
                            }
                            
                            all_articles.append(article_info)
                            count += 1
                            processed_in_page += 1
                            
                            print(f"âœ… [{count}/{target_count}] {title[:50]}... (ë³¸ë¬¸ {len(content)}ì)")
                            
                            if count >= target_count:
                                break
                        
                        print(f"ğŸ“„ í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num}ì—ì„œ {processed_in_page}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
                        
                        # ë‹¤ìŒ í˜ì´ì§€ë¡œ ì´ë™
                        try:
                            # ëª©í‘œ ê°œìˆ˜ì— ë„ë‹¬í–ˆìœ¼ë©´ ì¤‘ë‹¨
                            if count >= target_count:
                                break
                            
                            # ë°©ë²• 1: í˜ì´ì§€ë„¤ì´ì…˜ ë²„íŠ¼ í´ë¦­ ì‹œë„
                            next_selectors = [
                                'a[aria-label="ë‹¤ìŒ í˜ì´ì§€"]',
                                'button[aria-label="ë‹¤ìŒ í˜ì´ì§€"]',
                                '.pagination .next',
                                '.paging .next',
                                'a:has-text("ë‹¤ìŒ")',
                                'button:has-text("ë‹¤ìŒ")',
                                '.page-next',
                                '.btn-next',
                                f'a[href*="page={page_num + 1}"]',
                                '.pagination a:last-child',
                                '.paging a:last-child'
                            ]
                            
                            next_clicked = False
                            for selector in next_selectors:
                                try:
                                    next_button = page.locator(selector).first
                                    if await next_button.is_visible() and await next_button.is_enabled():
                                        await next_button.click()
                                        await page.wait_for_timeout(3000)
                                        
                                        # í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                        try:
                                            await page.wait_for_selector('li.ArticleList_item___OGQO a', timeout=10000)
                                            next_clicked = True
                                            page_num += 1
                                            print(f"âœ… í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num}ë¡œ ì´ë™ ì„±ê³µ (ë²„íŠ¼ í´ë¦­)")
                                            break
                                        except:
                                            print(f"  âŒ ë²„íŠ¼ í´ë¦­ í›„ í˜ì´ì§€ ë¡œë”© ì‹¤íŒ¨")
                                            continue
                                except Exception as next_error:
                                    continue
                            
                            # ë°©ë²• 2: ë²„íŠ¼ í´ë¦­ì´ ì‹¤íŒ¨í•˜ë©´ URL ì§ì ‘ ì¡°ì‘
                            if not next_clicked:
                                page_num += 1
                                base_url = url
                                if '?' in base_url:
                                    next_url = f"{base_url}&page={page_num}"
                                else:
                                    next_url = f"{base_url}?page={page_num}"
                                
                                print(f"ğŸ“„ í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num} URL ì§ì ‘ ì ‘ê·¼: {next_url}")
                                
                                try:
                                    await page.goto(next_url, wait_until="domcontentloaded", timeout=30000)
                                    await page.wait_for_timeout(3000)
                                    
                                    # í˜ì´ì§€ê°€ ì‹¤ì œë¡œ ë³€ê²½ë˜ì—ˆëŠ”ì§€ í™•ì¸
                                    await page.wait_for_selector('li.ArticleList_item___OGQO a', timeout=10000)
                                    print(f"âœ… í•œê²¨ë ˆ {category} í˜ì´ì§€ {page_num}ë¡œ ì´ë™ ì„±ê³µ (URL ì¡°ì‘)")
                                except Exception as url_error:
                                    print(f"âŒ í•œê²¨ë ˆ {category} URL ì§ì ‘ ì ‘ê·¼ ì‹¤íŒ¨: {url_error}")
                                    break
                                
                        except Exception as e:
                            print(f"âŒ í•œê²¨ë ˆ {category}: í˜ì´ì§€ ì´ë™ ì¤‘ ì˜¤ë¥˜ - {e}")
                            break
                            
                    except Exception as e:
                        print(f"âŒ í•œê²¨ë ˆ {category} í˜ì´ì§€ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        break
                
                await page.close()
                print(f"âœ… í•œê²¨ë ˆ {category}ì—ì„œ {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ Error crawling í•œê²¨ë ˆ {category}: {e}")
                if 'page' in locals():
                    await page.close()
                continue
        
        await browser.close()
    
    print(f"âœ… í•œê²¨ë ˆì—ì„œ ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_articles

if __name__ == "__main__":
    articles = asyncio.run(get_articles())
    print(f"í•œê²¨ë ˆ ì´ {len(articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    for article in articles:
        print(f"- {article['category']}: {article['title']}")