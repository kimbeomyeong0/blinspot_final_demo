# crawler/crawl_ytn.py

import os
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime
from utils.parser_common import get_html, clean_text
from playwright.async_api import async_playwright
import logging
import re

logger = logging.getLogger(__name__)

CATEGORY_URLS = {
    "ì •ì¹˜": [
        "https://www.ytn.co.kr/news/list.php?mcd=0101",
        "https://www.ytn.co.kr/news/list.php?mcd=0101&page=1",
        "https://www.ytn.co.kr/news/list.php?mcd=0101&page=2",
        "https://www.ytn.co.kr/news/list.php?mcd=0101&page=3",
        "https://www.ytn.co.kr/news/list.php?mcd=0101&page=4",
        "https://www.ytn.co.kr/news/list.php?mcd=0101&page=5",
        "https://www.ytn.co.kr/news/list.php?mcd=0101&page=6",
        "https://www.ytn.co.kr/news/list.php?mcd=0101&page=7",
        "https://www.ytn.co.kr/news/list.php?mcd=0101&page=8"
    ],
    "ì‚¬íšŒ": [
        "https://www.ytn.co.kr/news/list.php?mcd=0103",
        "https://www.ytn.co.kr/news/list.php?mcd=0103&page=1",
        "https://www.ytn.co.kr/news/list.php?mcd=0103&page=2",
        "https://www.ytn.co.kr/news/list.php?mcd=0103&page=3",
        "https://www.ytn.co.kr/news/list.php?mcd=0103&page=4",
        "https://www.ytn.co.kr/news/list.php?mcd=0103&page=5",
        "https://www.ytn.co.kr/news/list.php?mcd=0103&page=6",
        "https://www.ytn.co.kr/news/list.php?mcd=0103&page=7",
        "https://www.ytn.co.kr/news/list.php?mcd=0103&page=8"
    ],
    "ê²½ì œ": [
        "https://www.ytn.co.kr/news/list.php?mcd=0102",
        "https://www.ytn.co.kr/news/list.php?mcd=0102&page=1",
        "https://www.ytn.co.kr/news/list.php?mcd=0102&page=2",
        "https://www.ytn.co.kr/news/list.php?mcd=0102&page=3",
        "https://www.ytn.co.kr/news/list.php?mcd=0102&page=4",
        "https://www.ytn.co.kr/news/list.php?mcd=0102&page=5",
        "https://www.ytn.co.kr/news/list.php?mcd=0102&page=6",
        "https://www.ytn.co.kr/news/list.php?mcd=0102&page=7",
        "https://www.ytn.co.kr/news/list.php?mcd=0102&page=8"
    ],
}

async def extract_article_content(page, article_url):
    """YTN ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        # ê¸°ì¡´ í˜ì´ì§€ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ
        await page.goto(article_url, wait_until="domcontentloaded", timeout=20000)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        await page.wait_for_timeout(1000)
        
        html = await page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        # YTN ê¸°ì‚¬ ë³¸ë¬¸ ì…€ë ‰í„° ì‹œë„ (ì‹¤ì œ ì‚¬ì´íŠ¸ êµ¬ì¡° ê¸°ë°˜)
        content_selectors = [
            '.paragraph',  # ì‹¤ì œ ë³¸ë¬¸ ì˜ì—­
            '.content',    # ë³¸ë¬¸ ì»¨í…Œì´ë„ˆ
            '.news_view_wrap .inner',  # ë‰´ìŠ¤ ë³´ê¸° ì˜ì—­
            '.article-body',
            '.news-article-body',
            '.story-content',
            '.entry-content',
            '#article-body',
            '.article-content',
            '.text-content',
            '.content-body',
            '.detail-content',
            '.article-text',
            '.article_txt',
            '.article_content',
            '.news_content',
            '.text',
            '.story_text',
            '.article_wrap',
            '.article-wrap',
            '.news-content',
            '.story-body',
            '.content-text',
            '.view_content',
            '.news_view',
            '.article_view',
            '.content_view',
            '.detail_content',
            '.news_detail',
            '.article_detail',
            '.view_text',
            '.news_text',
            '.article_text'
        ]
        
        content = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                # ê´‘ê³ ë‚˜ ë¶ˆí•„ìš”í•œ í…ìŠ¤íŠ¸ ì œê±°
                for ad in content_elem.find_all(['script', 'style', 'iframe', 'ins']):
                    ad.decompose()
                
                text = content_elem.get_text(separator=' ', strip=True)
                # ê´‘ê³  í…ìŠ¤íŠ¸ ì œê±°
                text = re.sub(r'AD\s*', '', text)
                text = re.sub(r'\s+', ' ', text)
                
                if len(text) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ë©´ ì¦‰ì‹œ ë°˜í™˜
                    content = text
                    break
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
        if not content or len(content.strip()) < 50:
            content = "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        return content
        
    except Exception as e:
        logger.error(f"YTN ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - {article_url}: {e}")
        return "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

async def get_articles():
    all_articles = []
    
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ìµœì í™” ì„¤ì •
        browser = await p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-images',
                '--disable-web-security',
                '--disable-features=TranslateUI',
                '--disable-renderer-backgrounding',
                '--disable-background-timer-throttling',
                '--disable-default-apps',
                '--disable-sync',
                '--disable-translate',
                '--hide-scrollbars',
                '--mute-audio',
                '--no-first-run',
                '--no-default-browser-check',
                '--disable-component-update',
                '--disable-domain-reliability',
                '--disable-print-preview',
                '--disable-speech-api',
                '--disable-web-bluetooth',
                '--disable-client-side-phishing-detection',
                '--disable-hang-monitor',
                '--disable-prompt-on-repost',
                '--disable-breakpad',
                '--disable-dev-tools',
                '--disable-in-process-stack-traces',
                '--disable-histogram-customizer',
                '--disable-gl-extensions',
                '--disable-3d-apis',
                '--disable-accelerated-2d-canvas',
                '--disable-accelerated-jpeg-decoding',
                '--disable-accelerated-mjpeg-decode',
                '--disable-accelerated-video-decode',
            ]
        )
        
        for category, urls in CATEGORY_URLS.items():
            try:
                print(f"ğŸ” YTN {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
                
                count = 0
                target_count = 30
                processed_urls = set()
                
                for url_idx, url in enumerate(urls):
                    if count >= target_count:
                        break
                        
                    try:
                        print(f"ğŸ“„ YTN {category} URL {url_idx + 1}/{len(urls)} ì²˜ë¦¬ ì¤‘...")
                        
                        page = await browser.new_page()
                        
                        # í˜ì´ì§€ ìµœì í™” ì„¤ì •
                        await page.set_extra_http_headers({
                            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                            'Cache-Control': 'no-cache'
                        })
                        
                        # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ (ì„±ëŠ¥ í–¥ìƒ)
                        await page.route('**/*.{png,jpg,jpeg,gif,svg,ico,webp,css,woff,woff2,ttf,eot}', 
                                       lambda route: route.abort())
                        
                        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                        
                        # YTN ê¸°ì‚¬ ëª©ë¡ ì…€ë ‰í„° (ë” í¬ê´„ì ìœ¼ë¡œ)
                        selectors = [
                            "div.news_list div.title a",
                            "div.list_cont div.title a",
                            ".news_list .title a",
                            "ul.news_list li a",
                            "div.title a",
                            "a[href*='/_ln/']",  # YTN ê¸°ì‚¬ URL íŒ¨í„´
                            "a[href*='/news/']",
                            ".list_news a",
                            ".news_item a",
                            ".article_item a",
                            ".news_link",
                            "a[href*='ytn.co.kr']",
                            ".list_item a",
                            ".article_link a",
                            ".content_link a",
                            ".news-card a",
                            ".story-item a",
                            "li.news-item a",
                            ".news-list-item a",
                            "a[href*='article_view']",
                            ".item-title a",
                            ".headline-link a",
                            ".news-headline a",
                            "a[href*='view.php']"
                        ]
                        
                        articles = []
                        for selector in selectors:
                            elements = await page.query_selector_all(selector)
                            if elements:
                                for element in elements:
                                    try:
                                        href = await element.get_attribute('href')
                                        if href and href not in processed_urls:
                                            if href.startswith('/'):
                                                href = f"https://www.ytn.co.kr{href}"
                                            elif not href.startswith('http'):
                                                href = f"https://www.ytn.co.kr/{href}"
                                            
                                            # ì¤‘ë³µ ì²´í¬ë¥¼ ë” ê´€ëŒ€í•˜ê²Œ - URL ê¸°ë°˜ìœ¼ë¡œë§Œ ì²´í¬
                                            if href not in processed_urls:
                                                processed_urls.add(href)
                                                
                                                title = await element.text_content()
                                                if title and title.strip():
                                                    articles.append({
                                                        'title': title.strip(),
                                                        'url': href,
                                                        'category': category
                                                    })
                                                    
                                                    if len(articles) >= 30:
                                                        break
                                    except Exception as e:
                                        continue
                                break
                        
                        if not articles:
                            print(f"âŒ YTN {category} URL {url_idx + 1}: ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                            break
                        
                        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ (ìµœëŒ€ 50ë²ˆ)
                        max_clicks = 50
                        click_count = 0
                        consecutive_no_new_articles = 0
                        
                        # ë‹¤ì–‘í•œ ë”ë³´ê¸° ë²„íŠ¼ ì…€ë ‰í„° ì‹œë„
                        more_selectors = [
                            "button:has-text('ë”ë³´ê¸°')",
                            "a:has-text('ë”ë³´ê¸°')",
                            ".more_btn",
                            ".btn_more",
                            "#more_btn",
                            "button.more",
                            "a.more",
                            "[onclick*='more']",
                            "button:has-text('More')",
                            ".load_more",
                            ".more_list",
                            ".btn_load_more",
                            "button[data-more]",
                            "a[data-more]",
                            ".paging .next",
                            ".next_page",
                            "a:has-text('ë‹¤ìŒ')",
                            "button:has-text('ë‹¤ìŒ')",
                            ".pagination .next",
                            ".page-next",
                            ".more-news",
                            ".load-more-news",
                            "button[onclick*='more']",
                            "a[onclick*='more']",
                            ".btn-more-news",
                            ".more-articles",
                            "#loadMore",
                            ".load-more-btn",
                            "button:has-text('ê¸°ì‚¬ ë”ë³´ê¸°')",
                            "a:has-text('ê¸°ì‚¬ ë”ë³´ê¸°')"
                        ]
                        
                        while click_count < max_clicks and len(articles) < 30:
                            print(f"  ğŸ”„ YTN {category} URL {url_idx+1}: ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ({click_count+1}ë²ˆì§¸)")
                            
                            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì „ í˜„ì¬ ê¸°ì‚¬ ìˆ˜ ì €ì¥
                            before_click_count = len(articles)
                            
                            # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­
                            clicked = False
                            for selector in more_selectors:
                                try:
                                    await page.click(selector, timeout=3000)
                                    clicked = True
                                    break
                                except:
                                    continue
                            
                            if not clicked:
                                print(f"  ğŸ“„ YTN {category} URL {url_idx+1}: ë”ë³´ê¸° ë²„íŠ¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                                break
                            
                            # ìƒˆ ì½˜í…ì¸  ë¡œë”© ëŒ€ê¸°
                            await page.wait_for_timeout(5000)  # ë” ê¸´ ëŒ€ê¸° ì‹œê°„
                            
                            # ê¸°ì‚¬ ë§í¬ ë‹¤ì‹œ ìˆ˜ì§‘
                            for selector in selectors:
                                try:
                                    elements = await page.query_selector_all(selector)
                                    if elements:
                                        for element in elements:
                                            try:
                                                href = await element.get_attribute('href')
                                                if href and href not in processed_urls:
                                                    if href.startswith('/'):
                                                        href = f"https://www.ytn.co.kr{href}"
                                                    elif not href.startswith('http'):
                                                        href = f"https://www.ytn.co.kr/{href}"
                                                    
                                                    # ì¤‘ë³µ ì²´í¬ë¥¼ ë” ê´€ëŒ€í•˜ê²Œ - URL ê¸°ë°˜ìœ¼ë¡œë§Œ ì²´í¬
                                                    if href not in processed_urls:
                                                        processed_urls.add(href)
                                                        
                                                        title = await element.text_content()
                                                        if title and title.strip():
                                                            articles.append({
                                                                'title': title.strip(),
                                                                'url': href,
                                                                'category': category
                                                            })
                                                            
                                                            if len(articles) >= 30:
                                                                break
                                            except Exception as e:
                                                continue
                                        break
                                except Exception as e:
                                    continue
                            
                            # ìƒˆë¡œìš´ ê¸°ì‚¬ê°€ ì¶”ê°€ë˜ì—ˆëŠ”ì§€ í™•ì¸
                            after_click_count = len(articles)
                            if after_click_count > before_click_count:
                                consecutive_no_new_articles = 0
                                print(f"  ğŸ“„ YTN {category} URL {url_idx+1}: {after_click_count - before_click_count}ê°œ ìƒˆ ê¸°ì‚¬ ë°œê²¬")
                            else:
                                consecutive_no_new_articles += 1
                                print(f"  ğŸ“„ YTN {category} URL {url_idx+1}: ìƒˆë¡œìš´ ê¸°ì‚¬ ì—†ìŒ (ì—°ì† {consecutive_no_new_articles}ë²ˆ)")
                            
                            # ì—°ì†ìœ¼ë¡œ 5ë²ˆ ìƒˆ ê¸°ì‚¬ê°€ ì—†ìœ¼ë©´ ì¤‘ë‹¨
                            if consecutive_no_new_articles >= 5:
                                print(f"  ğŸ“„ YTN {category} URL {url_idx+1}: ì—°ì† 5ë²ˆ ìƒˆ ê¸°ì‚¬ ì—†ì–´ ì¤‘ë‹¨")
                                break
                            
                            click_count += 1
                        
                        print(f"âœ… YTN {category} URL {url_idx+1}ì—ì„œ {click_count}ë²ˆ í´ë¦­ ì™„ë£Œ")
                        
                        # ìˆ˜ì§‘ëœ ê¸°ì‚¬ë“¤ì˜ ë³¸ë¬¸ ì¶”ì¶œ
                        for i, article in enumerate(articles):
                            if count >= target_count:
                                break
                                
                            try:
                                # ë³¸ë¬¸ ì¶”ì¶œ
                                print(f"ğŸ“„ [{count+1}/{target_count}] {article['title']} - ë³¸ë¬¸ ì¶”ì¶œ ì¤‘...")
                                content = await extract_article_content(page, article_url=article['url'])
                                
                                final_article = {
                                    'title': article['title'],
                                    'url': article['url'],
                                    'category': category,
                                    'content': content,
                                    'published_at': datetime.now().isoformat(),
                                    'source': 'ytn'
                                }
                                
                                all_articles.append(final_article)
                                count += 1
                                
                                print(f"âœ… [{count}/{target_count}] {article['title'][:50]}... (ë³¸ë¬¸ {len(content)}ì)")
                                
                            except Exception as e:
                                print(f"âŒ YTN {category} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                continue
                        
                        await page.close()
                        
                    except Exception as e:
                        print(f"âŒ YTN {category} URL {url_idx + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        if 'page' in locals():
                            await page.close()
                        continue
                
                print(f"âœ… YTN {category}ì—ì„œ {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ Error crawling YTN {category}: {e}")
                if 'page' in locals():
                    await page.close()
                continue
        
        await browser.close()
    
    print(f"âœ… YTNì—ì„œ ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
    return all_articles

if __name__ == "__main__":
    asyncio.run(get_articles())