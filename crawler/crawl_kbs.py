# crawler/crawl_kbs.py

import os
import asyncio
from bs4 import BeautifulSoup
from datetime import datetime
from utils.parser_common import get_html, clean_text
from playwright.async_api import async_playwright
import logging
import re

logger = logging.getLogger(__name__)

# KBS ë‰´ìŠ¤ ì¹´í…Œê³ ë¦¬ë³„ URL (ì˜¬ë°”ë¥¸ ì¹´í…Œê³ ë¦¬ ì½”ë“œ)
CATEGORY_URLS = {
    "ì •ì¹˜": [
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003",  # ì •ì¹˜
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=1",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=2",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=3",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=4",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=5",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=6",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=7",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=8",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=9",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=10",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=11",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=12",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=13",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=14",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=15",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=16",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=17",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=18",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=19",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0003&page=20"
    ],
    "ì‚¬íšŒ": [
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004",  # ì‚¬íšŒ
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=1",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=2",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=3",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=4",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=5",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=6",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=7",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=8",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=9",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=10",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=11",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=12",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=13",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=14",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=15",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=16",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=17",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=18",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=19",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0004&page=20"
    ],
    "ê²½ì œ": [
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002",  # ê²½ì œ
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=1",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=2",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=3",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=4",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=5",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=6",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=7",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=8",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=9",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=10",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=11",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=12",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=13",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=14",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=15",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=16",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=17",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=18",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=19",
        "https://news.kbs.co.kr/news/pc/category/category.do?ctcd=0002&page=20"
    ]
}

async def extract_article_content(page, article_url):
    """KBS ê¸°ì‚¬ ë³¸ë¬¸ ì¶”ì¶œ"""
    try:
        # ê¸°ì¡´ í˜ì´ì§€ë¥¼ ì¬ì‚¬ìš©í•˜ì—¬ ë³¸ë¬¸ ì¶”ì¶œ (ìƒˆ íƒ­ ìƒì„±í•˜ì§€ ì•ŠìŒ)
        await page.goto(article_url, wait_until="domcontentloaded", timeout=20000)
        
        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
        await page.wait_for_timeout(500)
        
        html = await page.content()
        soup = BeautifulSoup(html, 'html.parser')
        
        # KBS ê¸°ì‚¬ ë³¸ë¬¸ ì…€ë ‰í„° ì‹œë„ (ìš°ì„ ìˆœìœ„ ìˆœ)
        content_selectors = [
            '#cont_newstext',
            '.article-body',
            '.news-article-body',
            '.story-content',
            '.entry-content',
            '#article-body',
            '.article-content',
            '.text-content',
            '.content-body',
            '.detail-content'
        ]
        
        content = ""
        for selector in content_selectors:
            content_elem = soup.select_one(selector)
            if content_elem:
                content = clean_text(content_elem.get_text())
                if len(content) > 100:  # ì¶©ë¶„í•œ ê¸¸ì´ë©´ ì¦‰ì‹œ ë°˜í™˜
                    break
        
        # ë³¸ë¬¸ì´ ë„ˆë¬´ ì§§ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€ ë°˜í™˜
        if not content or len(content.strip()) < 50:
            content = "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
            
        return content
        
    except Exception as e:
        logger.error(f"KBS ë³¸ë¬¸ ì¶”ì¶œ ì‹¤íŒ¨ - {article_url}: {e}")
        return "ë³¸ë¬¸ì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

async def get_articles():
    all_articles = []
    processed_urls = set()  # URL ê¸°ë°˜ ì¤‘ë³µ ì²´í¬ë¥¼ ìœ„í•œ ì§‘í•©
    
    async with async_playwright() as p:
        # ë¸Œë¼ìš°ì € ìµœì í™” ì„¤ì • (ì•ˆì „í•œ ì˜µì…˜ë§Œ ì‚¬ìš©)
        browser = await p.chromium.launch(
            headless=True,
            args=[
                '--no-sandbox',
                '--disable-dev-shm-usage',
                '--disable-blink-features=AutomationControlled',
                '--disable-extensions',
                '--disable-plugins',
                '--disable-images',  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™” (ì„±ëŠ¥ í–¥ìƒ)
                '--disable-web-security',
                '--disable-features=TranslateUI',
                '--disable-renderer-backgrounding',
                '--disable-backgrounding-occluded-windows',
                '--disable-background-timer-throttling',
                '--disable-default-apps',
                '--disable-sync',
                '--disable-translate',
                '--hide-scrollbars',
                '--mute-audio',
                '--no-first-run',
                '--no-default-browser-check',
                '--disable-component-update',
                '--disable-domain-reliability',
                '--disable-print-preview',
                '--disable-speech-api',
                '--disable-web-bluetooth',
                '--disable-client-side-phishing-detection',
                '--disable-hang-monitor',
                '--disable-prompt-on-repost',
                '--disable-breakpad',
                '--disable-dev-tools',
                '--disable-in-process-stack-traces',
                '--disable-histogram-customizer',
                '--disable-gl-extensions',
                '--disable-3d-apis',
                '--disable-accelerated-2d-canvas',
                '--disable-accelerated-jpeg-decoding',
                '--disable-accelerated-mjpeg-decode',
                '--disable-accelerated-video-decode',
            ]
        )
        
        for category, urls in CATEGORY_URLS.items():
            try:
                print(f"ğŸ” KBS {category} ì¹´í…Œê³ ë¦¬ í¬ë¡¤ë§ ì¤‘...")
                
                count = 0
                target_count = 30
                
                for url_idx, url in enumerate(urls):
                    if count >= target_count:
                        break
                        
                    try:
                        print(f"ğŸ“„ KBS {category} URL {url_idx + 1}/{len(urls)} ì²˜ë¦¬ ì¤‘...")
                        
                        page = await browser.new_page()
                        
                        # í˜ì´ì§€ ìµœì í™” ì„¤ì •
                        await page.set_extra_http_headers({
                            'Accept-Language': 'ko-KR,ko;q=0.9,en;q=0.8',
                            'Cache-Control': 'no-cache'
                        })
                        
                        # ë¶ˆí•„ìš”í•œ ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨ (ì„±ëŠ¥ í–¥ìƒ)
                        await page.route('**/*.{png,jpg,jpeg,gif,svg,ico,webp,css,woff,woff2,ttf,eot}', 
                                       lambda route: route.abort())
                        
                        await page.goto(url, wait_until="domcontentloaded", timeout=30000)
                        
                        # í˜ì´ì§€ ë¡œë”© ëŒ€ê¸°
                        try:
                            await page.wait_for_selector('a.box-content.flex-style', timeout=15000)
                        except:
                            print(f"âŒ KBS {category} URL {url_idx + 1}: ê¸°ì‚¬ ëª©ë¡ ë¡œë”© ì‹¤íŒ¨")
                            await page.close()
                            continue
                        
                        # í˜„ì¬ í˜ì´ì§€ì˜ ê¸°ì‚¬ ìˆ˜ì§‘
                        html = await page.content()
                        soup = BeautifulSoup(html, "html.parser")
                        
                        # KBS ê¸°ì‚¬ ëª©ë¡ ì…€ë ‰í„° (ë” í¬ê´„ì ìœ¼ë¡œ)
                        selectors = [
                            "a.box-content.flex-style",
                            ".box-content a",
                            ".news-item a",
                            ".article-item a",
                            "article a",
                            ".headline a",
                            "a[href*='/news/']",
                            ".news-list a",
                            ".article-list a",
                            ".list-item a",
                            "a[href*='kbs.co.kr']",
                            ".content-link a",
                            ".story-item a",
                            ".news-card a",
                            ".list-cont a",
                            "li a[href*='/news/view.do']",
                            ".article-link a",
                            ".news-link a",
                            ".item-link a",
                            "div.item a",
                            ".news-item-link",
                            "a[href*='view.do?ncd=']"
                        ]
                        
                        nodes = []
                        for selector in selectors:
                            nodes = soup.select(selector)
                            if nodes:
                                print(f"ğŸ“„ KBS {category} URL {url_idx + 1}ì—ì„œ '{selector}' ì…€ë ‰í„°ë¡œ {len(nodes)}ê°œ ìš”ì†Œ ë°œê²¬")
                                break
                        
                        if not nodes:
                            print(f"âŒ KBS {category} URL {url_idx + 1}: ê¸°ì‚¬ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤")
                            await page.close()
                            continue
                        
                        # í˜„ì¬ í˜ì´ì§€ì—ì„œ ì²˜ë¦¬ëœ ê¸°ì‚¬ ìˆ˜
                        processed_in_page = 0
                        
                        # ê° ê¸°ì‚¬ ì²˜ë¦¬
                        for node in nodes:
                            if count >= target_count:
                                break
                                
                            try:
                                link = node.get('href')
                                
                                # ì œëª© ì¶”ì¶œ (ë‹¤ì–‘í•œ ì…€ë ‰í„° ì‹œë„)
                                title_selectors = [
                                    '.title',
                                    '.headline',
                                    '.news-title',
                                    'h3',
                                    'h4',
                                    '.subject'
                                ]
                                
                                title = ""
                                for title_selector in title_selectors:
                                    title_elem = node.select_one(title_selector)
                                    if title_elem:
                                        title = clean_text(title_elem.get_text())
                                        break
                                
                                # ì œëª©ì´ ì—†ìœ¼ë©´ ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ ì¶”ì¶œ
                                if not title:
                                    title = clean_text(node.get_text())
                                
                                if not link or not title or len(title) < 3:
                                    continue
                                
                                # linkê°€ ë¦¬ìŠ¤íŠ¸ì¸ ê²½ìš° ì²« ë²ˆì§¸ ìš”ì†Œ ì‚¬ìš©
                                if isinstance(link, list):
                                    link = link[0] if link else ""
                                
                                # ìƒëŒ€ ê²½ë¡œë¥¼ ì ˆëŒ€ ê²½ë¡œë¡œ ë³€í™˜
                                if link.startswith('/'):
                                    link = f"https://news.kbs.co.kr{link}"
                                elif not link.startswith('http'):
                                    link = f"https://news.kbs.co.kr/{link}"
                                
                                # ì¤‘ë³µ ì œê±° (URL ê¸°ë°˜ìœ¼ë¡œë§Œ ì²´í¬)
                                if link in processed_urls:
                                    continue
                                
                                processed_urls.add(link)
                                
                                # ë³¸ë¬¸ ì¶”ì¶œ
                                print(f"ğŸ“„ [{count+1}/{target_count}] {title} - ë³¸ë¬¸ ì¶”ì¶œ ì¤‘...")
                                content = await extract_article_content(page, article_url=link)
                                
                                article = {
                                    'title': title,
                                    'url': link,
                                    'category': category,
                                    'content': content,
                                    'published_at': datetime.now().isoformat(),
                                    'source': 'kbs'
                                }
                                
                                all_articles.append(article)
                                count += 1
                                processed_in_page += 1
                                
                                print(f"âœ… [{count}/{target_count}] {title[:50]}... (ë³¸ë¬¸ {len(content)}ì)")
                                
                            except Exception as e:
                                print(f"âŒ KBS {category} ê¸°ì‚¬ ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                                continue
                        
                        print(f"ğŸ“„ KBS {category} URL {url_idx + 1}ì—ì„œ {processed_in_page}ê°œ ê¸°ì‚¬ ì²˜ë¦¬ ì™„ë£Œ")
                        await page.close()
                        
                    except Exception as e:
                        print(f"âŒ KBS {category} URL {url_idx + 1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                        if 'page' in locals():
                            await page.close()
                        continue
                
                print(f"âœ… KBS {category}ì—ì„œ {count}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
                
            except Exception as e:
                print(f"âŒ Error crawling KBS {category}: {e}")
                if 'page' in locals():
                    await page.close()
                continue
        
        print(f"âœ… KBSì—ì„œ ì´ {len(all_articles)}ê°œ ê¸°ì‚¬ ìˆ˜ì§‘ ì™„ë£Œ")
        return all_articles

if __name__ == "__main__":
    asyncio.run(get_articles())