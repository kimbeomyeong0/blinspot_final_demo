name: Daily News Crawling and Clustering

on:
  # ë§¤ì¼ UTC 15:00 (í•œêµ­ì‹œê°„ 00:00)ì— ì‹¤í–‰
  schedule:
    - cron: '0 15 * * *'
  
  # ìˆ˜ë™ ì‹¤í–‰ ê°€ëŠ¥
  workflow_dispatch:
    inputs:
      run_crawler:
        description: 'Run Crawler'
        required: true
        default: 'true'
        type: boolean
      run_cluster:
        description: 'Run Clustering'
        required: true
        default: 'true'
        type: boolean

jobs:
  # Job 1: í¬ë¡¤ë§ ë° ì—…ë¡œë“œ
  crawl-and-upload:
    runs-on: ubuntu-latest
    if: ${{ github.event.inputs.run_crawler != 'false' }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create data directories
      run: |
        mkdir -p crawler/data/raw
        mkdir -p backend/results
    
    - name: ğŸ” Step 1 - Run News Crawler
      working-directory: ./crawler
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        echo "ğŸ” [1/3] Starting news crawling..."
        echo "ğŸ“° Collecting articles from: í•œê²¨ë ˆ, ì¡°ì„ ì¼ë³´, KBS, YTN"
        python main_crawler.py
        echo "âœ… Crawling completed successfully!"
    
    - name: ğŸ“¤ Step 2 - Upload to Supabase
      working-directory: ./backend
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        echo "ğŸ“¤ [2/3] Uploading articles to Supabase..."
        python supabase_uploader.py
        echo "âœ… Upload completed successfully!"
        echo "ğŸ“Š Ready for clustering analysis..."

    - name: Archive crawled data
      uses: actions/upload-artifact@v3
      with:
        name: crawled-data-${{ github.run_number }}
        path: crawler/data/raw/*.json
        retention-days: 7

  # Job 2: í´ëŸ¬ìŠ¤í„°ë§ (í¬ë¡¤ë§ ì™„ë£Œ í›„ ì‹¤í–‰)
  clustering-analysis:
    runs-on: ubuntu-latest
    needs: crawl-and-upload  # í¬ë¡¤ë§ ì™„ë£Œ í›„ì—ë§Œ ì‹¤í–‰
    if: ${{ github.event.inputs.run_cluster != 'false' && (success() || github.event.inputs.run_crawler == 'false') }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
    
    - name: Cache pip dependencies
      uses: actions/cache@v3
      with:
        path: ~/.cache/pip
        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}
        restore-keys: |
          ${{ runner.os }}-pip-
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create results directory
      run: |
        mkdir -p backend/results
    
    - name: ğŸ§  Step 3 - Run Clustering Analysis
      working-directory: ./backend
      env:
        SUPABASE_URL: ${{ secrets.SUPABASE_URL }}
        SUPABASE_ANON_KEY: ${{ secrets.SUPABASE_ANON_KEY }}
        OPENAI_API_KEY: ${{ secrets.OPENAI_API_KEY }}
      run: |
        echo "ğŸ§  [3/3] Starting AI clustering analysis..."
        echo "ğŸ” Analyzing news articles for similar issues..."
        echo "âš–ï¸ Calculating bias distribution..."
        python main_cluster.py
        echo "âœ… Clustering analysis completed!"
        echo "ğŸ‰ New issues available in BlindSpot app!"
    
    - name: Archive clustering results
      uses: actions/upload-artifact@v3
      with:
        name: clustering-results-${{ github.run_number }}
        path: |
          backend/results/*.json
          backend/results/*.txt
        retention-days: 30
    
    - name: ğŸ‰ Summary
      run: |
        echo "ğŸ‰ Daily news analysis completed successfully!"
        echo ""
        echo "ğŸ“Š Process Summary:"
        echo "   1. âœ… News Crawling (í•œê²¨ë ˆ, ì¡°ì„ ì¼ë³´, KBS, YTN)"
        echo "   2. âœ… Data Upload to Supabase"
        echo "   3. âœ… AI-powered Issue Clustering"
        echo "   4. âœ… Bias Analysis Complete"
        echo ""
        echo "ğŸŒ Check your BlindSpot app for new issues!"
        echo "ğŸ“ Results archived as artifacts for 30 days" 